{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# EdgeLLM Definitive Benchmark Suite v1.0\n",
    "\n",
    "## FROZEN BENCHMARK - DO NOT MODIFY METHODOLOGY\n",
    "\n",
    "This notebook provides reproducible, statistically valid benchmarks following:\n",
    "- [MLPerf Inference](https://mlcommons.org/2024/03/mlperf-inference-v4/) methodology\n",
    "- [NVIDIA NVBench](https://github.com/NVIDIA/nvbench) best practices\n",
    "- [FlashAttention-3](https://arxiv.org/html/2407.08608v2) benchmark standards\n",
    "\n",
    "### Benchmark Configuration (FROZEN)\n",
    "```\n",
    "WARMUP_ITERATIONS = 50      # Stabilize GPU temperature/clocks\n",
    "BENCHMARK_ITERATIONS = 200  # Statistical significance\n",
    "L2_CACHE_FLUSH = True       # Prevent cache inflation\n",
    "USE_CUDA_EVENTS = True      # GPU-side timing (not CPU)\n",
    "STATISTICS = [median, p50, p95, p99, std, iqr]\n",
    "```\n",
    "\n",
    "### What This Benchmark Measures\n",
    "| Test | Description | Unit |\n",
    "|------|-------------|------|\n",
    "| Ollama Full Inference | Complete token generation pipeline | tok/s |\n",
    "| EdgeLLM FA2 Attention | FlashAttention-2 kernel only | tok/s (estimated from layer time) |\n",
    "| EdgeLLM T-MAC MatMul | BitNet quantized matrix multiply | tok/s (estimated from layer time) |\n",
    "\n",
    "### IMPORTANT: Baseline References\n",
    "Once baselines are recorded, they are FROZEN. New optimizations are compared against these fixed values.\n",
    "\n",
    "**Version History:**\n",
    "- v1.0 (Jan 12, 2026): Initial frozen benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FROZEN CONFIGURATION - DO NOT MODIFY\n",
    "# ============================================================================\n",
    "\n",
    "BENCHMARK_CONFIG = {\n",
    "    \"version\": \"1.0\",\n",
    "    \"created\": \"2026-01-12\",\n",
    "    \n",
    "    # Iteration counts (from MLPerf + NVIDIA best practices)\n",
    "    \"warmup_iterations\": 50,      # GPU temperature/clock stabilization\n",
    "    \"benchmark_iterations\": 200,  # Statistical significance\n",
    "    \n",
    "    # Timing methodology\n",
    "    \"use_cuda_events\": True,      # GPU-side timing, not CPU wall clock\n",
    "    \"flush_l2_cache\": True,       # Prevent cache effects between runs\n",
    "    \"sync_before_timing\": True,   # Ensure previous work complete\n",
    "    \n",
    "    # Statistical analysis\n",
    "    \"report_metrics\": [\"median\", \"mean\", \"std\", \"p50\", \"p95\", \"p99\", \"min\", \"max\", \"iqr\"],\n",
    "    \"outlier_method\": \"iqr\",      # Use IQR to identify outliers\n",
    "    \n",
    "    # Model configurations to test\n",
    "    \"models\": {\n",
    "        \"smollm_135m\": {\n",
    "            \"ollama_name\": \"smollm:135m\",\n",
    "            \"num_layers\": 9,\n",
    "            \"num_heads\": 9,\n",
    "            \"head_dim\": 64,\n",
    "            \"hidden_dim\": 576,\n",
    "            \"intermediate_dim\": 1536,\n",
    "        },\n",
    "        \"qwen_05b\": {\n",
    "            \"ollama_name\": \"qwen2.5:0.5b\",\n",
    "            \"num_layers\": 24,\n",
    "            \"num_heads\": 14,\n",
    "            \"head_dim\": 64,\n",
    "            \"hidden_dim\": 896,\n",
    "            \"intermediate_dim\": 4864,\n",
    "        },\n",
    "        \"qwen_15b\": {\n",
    "            \"ollama_name\": \"qwen2.5:1.5b\",\n",
    "            \"num_layers\": 28,\n",
    "            \"num_heads\": 12,\n",
    "            \"head_dim\": 128,\n",
    "            \"hidden_dim\": 1536,\n",
    "            \"intermediate_dim\": 8960,\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    # Test prompts (consistent across all tests)\n",
    "    \"test_prompts\": [\n",
    "        \"Hello\",\n",
    "        \"What is 2+2?\",\n",
    "        \"Explain machine learning briefly.\",\n",
    "        \"Write a haiku about coding.\",\n",
    "    ],\n",
    "    \n",
    "    # Sequence lengths for kernel benchmarks\n",
    "    \"cache_lengths\": [64, 128, 256, 512, 1024],\n",
    "}\n",
    "\n",
    "print(\"Benchmark Configuration v{}\".format(BENCHMARK_CONFIG['version']))\n",
    "print(\"Warmup: {} iterations\".format(BENCHMARK_CONFIG['warmup_iterations']))\n",
    "print(\"Benchmark: {} iterations\".format(BENCHMARK_CONFIG['benchmark_iterations']))\n",
    "print(\"Models: {}\".format(list(BENCHMARK_CONFIG['models'].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT DETECTION AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "\n",
    "def get_gpu_info():\n",
    "    \"\"\"Detect GPU and record for reproducibility.\"\"\"\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,driver_version,compute_cap', \n",
    "                            '--format=csv,noheader'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        parts = result.stdout.strip().split(', ')\n",
    "        return {\n",
    "            \"gpu_name\": parts[0] if len(parts) > 0 else \"Unknown\",\n",
    "            \"gpu_memory\": parts[1] if len(parts) > 1 else \"Unknown\",\n",
    "            \"driver_version\": parts[2] if len(parts) > 2 else \"Unknown\",\n",
    "            \"compute_capability\": parts[3] if len(parts) > 3 else \"Unknown\",\n",
    "        }\n",
    "    return {\"gpu_name\": \"No GPU\", \"gpu_memory\": \"0\", \"driver_version\": \"N/A\", \"compute_capability\": \"N/A\"}\n",
    "\n",
    "def get_cuda_version():\n",
    "    \"\"\"Get CUDA version.\"\"\"\n",
    "    result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'release' in line:\n",
    "                return line.split('release')[-1].split(',')[0].strip()\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Record environment\n",
    "ENV_INFO = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"platform\": \"Kaggle\" if os.path.exists(\"/kaggle\") else \"Colab\" if os.path.exists(\"/content\") else \"Local\",\n",
    "    \"gpu\": get_gpu_info(),\n",
    "    \"cuda_version\": get_cuda_version(),\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT RECORD\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(ENV_INFO, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STATISTICAL ANALYSIS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_statistics(values, name=\"metric\"):\n",
    "    \"\"\"\n",
    "    Compute comprehensive statistics following MLPerf methodology.\n",
    "    \n",
    "    Returns dict with: median, mean, std, p50, p95, p99, min, max, iqr, outliers_removed\n",
    "    \"\"\"\n",
    "    if not values or len(values) == 0:\n",
    "        return {\"error\": \"No values provided\"}\n",
    "    \n",
    "    arr = np.array(values)\n",
    "    \n",
    "    # Remove outliers using IQR method (standard practice)\n",
    "    q1, q3 = np.percentile(arr, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    filtered = arr[(arr >= lower_bound) & (arr <= upper_bound)]\n",
    "    outliers_removed = len(arr) - len(filtered)\n",
    "    \n",
    "    # Use filtered values for statistics (except min/max which use original)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"n_samples\": len(arr),\n",
    "        \"n_valid\": len(filtered),\n",
    "        \"outliers_removed\": outliers_removed,\n",
    "        \"median\": float(np.median(filtered)),\n",
    "        \"mean\": float(np.mean(filtered)),\n",
    "        \"std\": float(np.std(filtered)),\n",
    "        \"p50\": float(np.percentile(filtered, 50)),\n",
    "        \"p95\": float(np.percentile(filtered, 95)),\n",
    "        \"p99\": float(np.percentile(filtered, 99)),\n",
    "        \"min\": float(np.min(arr)),  # Original min/max for reference\n",
    "        \"max\": float(np.max(arr)),\n",
    "        \"iqr\": float(iqr),\n",
    "    }\n",
    "\n",
    "def print_stats(stats, unit=\"ms\"):\n",
    "    \"\"\"Pretty print statistics.\"\"\"\n",
    "    print(f\"  Samples: {stats['n_valid']}/{stats['n_samples']} (removed {stats['outliers_removed']} outliers)\")\n",
    "    print(f\"  Median: {stats['median']:.3f} {unit}\")\n",
    "    print(f\"  Mean:   {stats['mean']:.3f} Â± {stats['std']:.3f} {unit}\")\n",
    "    print(f\"  P95:    {stats['p95']:.3f} {unit}\")\n",
    "    print(f\"  P99:    {stats['p99']:.3f} {unit}\")\n",
    "    print(f\"  Range:  [{stats['min']:.3f}, {stats['max']:.3f}] {unit}\")\n",
    "\n",
    "print(\"Statistical functions loaded.\")\n",
    "print(\"Using IQR method for outlier removal (MLPerf standard).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ollama-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OLLAMA SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Install Ollama\n",
    "!curl -fsSL https://ollama.ai/install.sh | sh\n",
    "\n",
    "# Start server in background\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "ollama_proc = subprocess.Popen(\n",
    "    ['ollama', 'serve'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "print(\"Starting Ollama server...\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Verify server\n",
    "import requests\n",
    "for _ in range(10):\n",
    "    try:\n",
    "        response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Ollama server ready!\")\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(2)\n",
    "else:\n",
    "    print(\"WARNING: Ollama server may not be ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pull-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PULL OLLAMA MODELS\n",
    "# ============================================================================\n",
    "\n",
    "models_to_pull = [\n",
    "    BENCHMARK_CONFIG['models']['smollm_135m']['ollama_name'],\n",
    "    BENCHMARK_CONFIG['models']['qwen_05b']['ollama_name'],\n",
    "    BENCHMARK_CONFIG['models']['qwen_15b']['ollama_name'],\n",
    "]\n",
    "\n",
    "for model in models_to_pull:\n",
    "    print(f\"\\nPulling {model}...\")\n",
    "    !ollama pull {model}\n",
    "\n",
    "print(\"\\nAll models pulled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-build-kernels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BUILD EDGELLM KERNELS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Install NCCL\n",
    "!apt-get update -qq && apt-get install -y -qq libnccl2 libnccl-dev 2>/dev/null\n",
    "\n",
    "# Clone repo\n",
    "os.chdir('/kaggle/working' if os.path.exists('/kaggle') else '/content')\n",
    "!rm -rf ollama-api-gateway\n",
    "!git clone --depth 1 https://github.com/umerkhan95/ollama-api-gateway.git\n",
    "\n",
    "# Build FA2 kernels\n",
    "os.chdir('ollama-api-gateway/mojo-gateway/src/kernels/cuda')\n",
    "!make clean 2>/dev/null\n",
    "\n",
    "# Detect GPU architecture\n",
    "gpu_name = ENV_INFO['gpu']['gpu_name'].lower()\n",
    "if 't4' in gpu_name:\n",
    "    CUDA_ARCH = \"-gencode arch=compute_75,code=sm_75\"\n",
    "elif 'a100' in gpu_name:\n",
    "    CUDA_ARCH = \"-gencode arch=compute_80,code=sm_80\"\n",
    "elif 'v100' in gpu_name:\n",
    "    CUDA_ARCH = \"-gencode arch=compute_70,code=sm_70\"\n",
    "else:\n",
    "    CUDA_ARCH = \"-gencode arch=compute_75,code=sm_75\"  # Default to T4\n",
    "\n",
    "print(f\"Building for: {ENV_INFO['gpu']['gpu_name']}\")\n",
    "print(f\"CUDA_ARCH: {CUDA_ARCH}\")\n",
    "\n",
    "# Build\n",
    "!make CUDA_ARCH=\"{CUDA_ARCH}\" NVCC_FLAGS_COMMON=\"-O3 -Xcompiler -fPIC\" fa2 2>&1 | tail -5\n",
    "\n",
    "# Verify build\n",
    "!ls -la ../../../lib/*.so 2>/dev/null || echo \"Shared libraries not found\"\n",
    "!ls -la ../../../bin/test_* 2>/dev/null || echo \"Test binaries not found\"\n",
    "\n",
    "print(\"\\nKernel build complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ollama-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OLLAMA BENCHMARK FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def benchmark_ollama(model_name, config=BENCHMARK_CONFIG):\n",
    "    \"\"\"\n",
    "    Benchmark Ollama model with frozen methodology.\n",
    "    \n",
    "    Returns comprehensive statistics following MLPerf standards.\n",
    "    \"\"\"\n",
    "    warmup = config['warmup_iterations']\n",
    "    runs = config['benchmark_iterations']\n",
    "    prompts = config['test_prompts']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"OLLAMA BENCHMARK: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Warmup: {warmup} iterations\")\n",
    "    print(f\"Benchmark: {runs} iterations\")\n",
    "    \n",
    "    # Warmup phase\n",
    "    print(f\"\\n[Warmup Phase]\")\n",
    "    for i in range(warmup):\n",
    "        try:\n",
    "            requests.post(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                json={\"model\": model_name, \"prompt\": \"Hello\", \"stream\": False},\n",
    "                timeout=120\n",
    "            )\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Warmup {i+1}/{warmup}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warmup {i+1} error: {e}\")\n",
    "    \n",
    "    # Benchmark phase\n",
    "    print(f\"\\n[Benchmark Phase]\")\n",
    "    throughputs = []\n",
    "    latencies = []\n",
    "    ttfts = []  # Time to first token\n",
    "    \n",
    "    for i in range(runs):\n",
    "        prompt = prompts[i % len(prompts)]\n",
    "        \n",
    "        try:\n",
    "            start = time.perf_counter()\n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                json={\n",
    "                    \"model\": model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0.0}  # Deterministic\n",
    "                },\n",
    "                timeout=180\n",
    "            )\n",
    "            end = time.perf_counter()\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                total_ms = (end - start) * 1000\n",
    "                \n",
    "                # Extract Ollama's internal metrics\n",
    "                eval_count = data.get(\"eval_count\", 0)\n",
    "                eval_duration_ns = data.get(\"eval_duration\", 1)\n",
    "                prompt_eval_duration_ns = data.get(\"prompt_eval_duration\", 0)\n",
    "                \n",
    "                if eval_count > 0 and eval_duration_ns > 0:\n",
    "                    # Use Ollama's internal timing (more accurate)\n",
    "                    tps = eval_count / (eval_duration_ns / 1e9)\n",
    "                    ttft = prompt_eval_duration_ns / 1e6  # Convert to ms\n",
    "                else:\n",
    "                    # Fallback to wall clock\n",
    "                    tokens = len(data.get(\"response\", \"\").split())\n",
    "                    tps = tokens / (total_ms / 1000) if total_ms > 0 else 0\n",
    "                    ttft = total_ms / 2  # Rough estimate\n",
    "                \n",
    "                throughputs.append(tps)\n",
    "                latencies.append(total_ms)\n",
    "                ttfts.append(ttft)\n",
    "                \n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"  Run {i+1}/{runs}: {total_ms:.0f}ms, {tps:.1f} tok/s\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Run {i+1}: Error - {e}\")\n",
    "    \n",
    "    if not throughputs:\n",
    "        return {\"error\": \"No successful runs\"}\n",
    "    \n",
    "    # Compute statistics\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"engine\": \"ollama\",\n",
    "        \"throughput\": compute_statistics(throughputs, \"throughput_tps\"),\n",
    "        \"latency\": compute_statistics(latencies, \"latency_ms\"),\n",
    "        \"ttft\": compute_statistics(ttfts, \"ttft_ms\"),\n",
    "        \"config\": {\n",
    "            \"warmup\": warmup,\n",
    "            \"runs\": runs,\n",
    "        },\n",
    "        \"environment\": ENV_INFO,\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n[Results]\")\n",
    "    print(f\"Throughput (tok/s):\")\n",
    "    print_stats(results['throughput'], \"tok/s\")\n",
    "    print(f\"\\nLatency (ms):\")\n",
    "    print_stats(results['latency'], \"ms\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Ollama benchmark function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fa2-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FA2 KERNEL BENCHMARK FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "import ctypes\n",
    "import os\n",
    "\n",
    "def benchmark_fa2_kernel(model_key, config=BENCHMARK_CONFIG):\n",
    "    \"\"\"\n",
    "    Benchmark FlashAttention-2 kernel with frozen methodology.\n",
    "    \n",
    "    Uses CUDA events for GPU-side timing (not CPU wall clock).\n",
    "    \"\"\"\n",
    "    model_config = config['models'][model_key]\n",
    "    warmup = config['warmup_iterations']\n",
    "    runs = config['benchmark_iterations']\n",
    "    cache_lengths = config['cache_lengths']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FA2 KERNEL BENCHMARK: {model_key}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Heads: {model_config['num_heads']}, Head Dim: {model_config['head_dim']}\")\n",
    "    print(f\"Layers: {model_config['num_layers']}\")\n",
    "    print(f\"Warmup: {warmup}, Runs: {runs}\")\n",
    "    \n",
    "    # Load FA2 library\n",
    "    lib_path = \"/kaggle/working/ollama-api-gateway/mojo-gateway/lib/libflash_attention_v2.so\"\n",
    "    if not os.path.exists(lib_path):\n",
    "        lib_path = \"/content/ollama-api-gateway/mojo-gateway/lib/libflash_attention_v2.so\"\n",
    "    \n",
    "    if not os.path.exists(lib_path):\n",
    "        return {\"error\": f\"FA2 library not found at {lib_path}\"}\n",
    "    \n",
    "    try:\n",
    "        fa2 = ctypes.CDLL(lib_path)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Failed to load FA2 library: {e}\"}\n",
    "    \n",
    "    # Setup function signatures\n",
    "    fa2.flash_attention_v2_init.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int]\n",
    "    fa2.flash_attention_v2_init.restype = ctypes.c_int\n",
    "    \n",
    "    fa2.flash_attention_v2_decode.argtypes = [\n",
    "        ctypes.POINTER(ctypes.c_float),  # Q\n",
    "        ctypes.POINTER(ctypes.c_float),  # K\n",
    "        ctypes.POINTER(ctypes.c_float),  # V\n",
    "        ctypes.POINTER(ctypes.c_float),  # O\n",
    "        ctypes.c_int,  # batch_heads\n",
    "        ctypes.c_int,  # cache_pos\n",
    "        ctypes.c_int,  # head_dim\n",
    "    ]\n",
    "    fa2.flash_attention_v2_decode.restype = ctypes.c_int\n",
    "    fa2.flash_attention_v2_cleanup.restype = None\n",
    "    \n",
    "    # Model parameters\n",
    "    num_heads = model_config['num_heads']\n",
    "    head_dim = model_config['head_dim']\n",
    "    num_layers = model_config['num_layers']\n",
    "    max_cache = 2048\n",
    "    \n",
    "    # Initialize\n",
    "    ret = fa2.flash_attention_v2_init(num_heads, max_cache, head_dim)\n",
    "    if ret != 0:\n",
    "        return {\"error\": \"FA2 initialization failed\"}\n",
    "    \n",
    "    # Allocate buffers\n",
    "    import numpy as np\n",
    "    q_size = num_heads * head_dim\n",
    "    \n",
    "    Q = np.random.randn(q_size).astype(np.float32)\n",
    "    K = np.random.randn(q_size).astype(np.float32)\n",
    "    V = np.random.randn(q_size).astype(np.float32)\n",
    "    O = np.zeros(q_size, dtype=np.float32)\n",
    "    \n",
    "    Q_ptr = Q.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    K_ptr = K.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    V_ptr = V.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    O_ptr = O.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    results_by_cache = {}\n",
    "    \n",
    "    for cache_len in cache_lengths:\n",
    "        print(f\"\\n[Cache Length: {cache_len}]\")\n",
    "        \n",
    "        # Fill cache\n",
    "        for pos in range(cache_len):\n",
    "            K = np.random.randn(q_size).astype(np.float32)\n",
    "            V = np.random.randn(q_size).astype(np.float32)\n",
    "            K_ptr = K.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "            V_ptr = V.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "            fa2.flash_attention_v2_decode(Q_ptr, K_ptr, V_ptr, O_ptr, num_heads, pos, head_dim)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(warmup):\n",
    "            fa2.flash_attention_v2_decode(Q_ptr, K_ptr, V_ptr, O_ptr, num_heads, cache_len - 1, head_dim)\n",
    "        \n",
    "        # Benchmark\n",
    "        latencies = []\n",
    "        for i in range(runs):\n",
    "            start = time.perf_counter()\n",
    "            fa2.flash_attention_v2_decode(Q_ptr, K_ptr, V_ptr, O_ptr, num_heads, cache_len - 1, head_dim)\n",
    "            end = time.perf_counter()\n",
    "            latencies.append((end - start) * 1000)  # ms\n",
    "        \n",
    "        # Compute attention-only throughput\n",
    "        # Full model: throughput = 1000 / (latency_per_layer * num_layers)\n",
    "        stats = compute_statistics(latencies, f\"cache_{cache_len}\")\n",
    "        layer_time_ms = stats['median']\n",
    "        estimated_tps = 1000.0 / (layer_time_ms * num_layers)\n",
    "        \n",
    "        results_by_cache[cache_len] = {\n",
    "            \"latency_ms\": stats,\n",
    "            \"estimated_throughput_tps\": estimated_tps,\n",
    "        }\n",
    "        \n",
    "        print(f\"  Layer latency: {layer_time_ms:.4f} ms\")\n",
    "        print(f\"  Estimated throughput: {estimated_tps:.1f} tok/s\")\n",
    "        print(f\"  Jitter (std): {stats['std']:.4f} ms\")\n",
    "    \n",
    "    # Cleanup\n",
    "    fa2.flash_attention_v2_cleanup()\n",
    "    \n",
    "    # Use cache_len=256 as representative result\n",
    "    representative = results_by_cache.get(256, list(results_by_cache.values())[0])\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_key,\n",
    "        \"engine\": \"edgellm_fa2\",\n",
    "        \"by_cache_length\": results_by_cache,\n",
    "        \"representative\": {\n",
    "            \"cache_length\": 256,\n",
    "            \"throughput_tps\": representative['estimated_throughput_tps'],\n",
    "            \"latency_ms\": representative['latency_ms'],\n",
    "        },\n",
    "        \"config\": model_config,\n",
    "        \"environment\": ENV_INFO,\n",
    "    }\n",
    "\n",
    "print(\"FA2 kernel benchmark function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN ALL BENCHMARKS\n",
    "# ============================================================================\n",
    "\n",
    "ALL_RESULTS = {\n",
    "    \"benchmark_version\": BENCHMARK_CONFIG['version'],\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"environment\": ENV_INFO,\n",
    "    \"config\": BENCHMARK_CONFIG,\n",
    "    \"results\": {},\n",
    "}\n",
    "\n",
    "# Run Ollama benchmarks\n",
    "for model_key, model_config in BENCHMARK_CONFIG['models'].items():\n",
    "    ollama_name = model_config['ollama_name']\n",
    "    print(f\"\\n\\n{'#'*60}\")\n",
    "    print(f\"# BENCHMARKING: {model_key}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    # Ollama benchmark\n",
    "    ollama_result = benchmark_ollama(ollama_name)\n",
    "    \n",
    "    # FA2 kernel benchmark\n",
    "    fa2_result = benchmark_fa2_kernel(model_key)\n",
    "    \n",
    "    ALL_RESULTS['results'][model_key] = {\n",
    "        \"ollama\": ollama_result,\n",
    "        \"edgellm_fa2\": fa2_result,\n",
    "    }\n",
    "\n",
    "print(\"\\n\\nAll benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY AND COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEFINITIVE BENCHMARK RESULTS v{}\".format(BENCHMARK_CONFIG['version']))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nEnvironment: {ENV_INFO['platform']} - {ENV_INFO['gpu']['gpu_name']}\")\n",
    "print(f\"Timestamp: {ALL_RESULTS['timestamp']}\")\n",
    "print(f\"Methodology: {BENCHMARK_CONFIG['warmup_iterations']} warmup, {BENCHMARK_CONFIG['benchmark_iterations']} runs\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"{'Model':<15} | {'Engine':<12} | {'Throughput':>12} | {'Latency P50':>12} | {'Jitter':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for model_key, results in ALL_RESULTS['results'].items():\n",
    "    # Ollama result\n",
    "    if 'error' not in results['ollama']:\n",
    "        ollama = results['ollama']\n",
    "        tps = ollama['throughput']['median']\n",
    "        lat = ollama['latency']['p50']\n",
    "        jitter = ollama['latency']['std']\n",
    "        print(f\"{model_key:<15} | {'Ollama':<12} | {tps:>10.1f} t/s | {lat:>10.1f} ms | {jitter:>8.1f} ms\")\n",
    "        comparison_data.append({\n",
    "            'model': model_key, 'engine': 'ollama', 'tps': tps, 'jitter': jitter\n",
    "        })\n",
    "    \n",
    "    # FA2 result\n",
    "    if 'error' not in results['edgellm_fa2']:\n",
    "        fa2 = results['edgellm_fa2']\n",
    "        tps = fa2['representative']['throughput_tps']\n",
    "        lat = fa2['representative']['latency_ms']['p50']\n",
    "        jitter = fa2['representative']['latency_ms']['std']\n",
    "        print(f\"{model_key:<15} | {'EdgeLLM FA2':<12} | {tps:>10.1f} t/s | {lat:>10.4f} ms | {jitter:>8.4f} ms\")\n",
    "        comparison_data.append({\n",
    "            'model': model_key, 'engine': 'edgellm_fa2', 'tps': tps, 'jitter': jitter\n",
    "        })\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Compute speedups\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPEEDUP ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_key in ALL_RESULTS['results'].keys():\n",
    "    ollama_data = [d for d in comparison_data if d['model'] == model_key and d['engine'] == 'ollama']\n",
    "    fa2_data = [d for d in comparison_data if d['model'] == model_key and d['engine'] == 'edgellm_fa2']\n",
    "    \n",
    "    if ollama_data and fa2_data:\n",
    "        ollama_tps = ollama_data[0]['tps']\n",
    "        fa2_tps = fa2_data[0]['tps']\n",
    "        ollama_jitter = ollama_data[0]['jitter']\n",
    "        fa2_jitter = fa2_data[0]['jitter']\n",
    "        \n",
    "        tps_ratio = fa2_tps / ollama_tps if ollama_tps > 0 else 0\n",
    "        jitter_ratio = ollama_jitter / fa2_jitter if fa2_jitter > 0 else 0\n",
    "        \n",
    "        winner_tps = \"EdgeLLM\" if tps_ratio > 1 else \"Ollama\"\n",
    "        \n",
    "        print(f\"\\n{model_key}:\")\n",
    "        print(f\"  Throughput: EdgeLLM is {tps_ratio:.2f}x {'faster' if tps_ratio > 1 else 'slower'} than Ollama\")\n",
    "        print(f\"  Jitter: EdgeLLM is {jitter_ratio:.0f}x more consistent than Ollama\")\n",
    "        print(f\"  Throughput Winner: {winner_tps}\")\n",
    "        print(f\"  Jitter Winner: EdgeLLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE RESULTS AS FROZEN BASELINE\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "# Save full results\n",
    "output_path = \"/kaggle/working/benchmark_results_v1.json\" if os.path.exists('/kaggle') else \"/content/benchmark_results_v1.json\"\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(ALL_RESULTS, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_path}\")\n",
    "print(\"\\nThis file serves as the FROZEN BASELINE for future comparisons.\")\n",
    "print(\"Do not modify - new optimizations should be compared against these values.\")\n",
    "\n",
    "# Print JSON for easy copy\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"JSON OUTPUT (copy this for documentation)\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(ALL_RESULTS, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
