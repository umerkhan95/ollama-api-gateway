{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashAttention-2 Benchmark\n",
    "\n",
    "Comparing attention implementations on Tesla T4:\n",
    "- **FP32 Flash Attention** - Basic CUDA implementation\n",
    "- **INT8 Tensor Core** - WMMA-based INT8 attention\n",
    "- **FlashAttention-2** - Tiled with online softmax\n",
    "\n",
    "**Target:** 630+ tok/s (vs Ollama 423 tok/s baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,compute_cap,memory.total',\n",
    "                        '--format=csv,noheader'], capture_output=True, text=True)\n",
    "gpu_info = result.stdout.strip()\n",
    "print(f\"GPU: {gpu_info}\")\n",
    "\n",
    "parts = gpu_info.split(', ')\n",
    "GPU_NAME = parts[0] if len(parts) > 0 else 'Unknown'\n",
    "COMPUTE_CAP = parts[1] if len(parts) > 1 else '0.0'\n",
    "print(f\"Compute Capability: {COMPUTE_CAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!rm -rf ollama-api-gateway\n",
    "!git clone --depth 1 https://github.com/umerkhan95/ollama-api-gateway.git\n",
    "%cd ollama-api-gateway/mojo-gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build All Attention Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd src/kernels/cuda\n",
    "!make clean\n",
    "\n",
    "# Build all attention variants for T4\n",
    "!make CUDA_ARCH=\"-gencode arch=compute_75,code=sm_75\" \\\n",
    "      NVCC_FLAGS_COMMON=\"-O3 -Xcompiler -fPIC -Xcompiler -Wall\" \\\n",
    "      flash\n",
    "\n",
    "!make CUDA_ARCH=\"-gencode arch=compute_75,code=sm_75\" \\\n",
    "      NVCC_FLAGS_COMMON=\"-O3 -Xcompiler -fPIC -Xcompiler -Wall --expt-relaxed-constexpr\" \\\n",
    "      int8\n",
    "\n",
    "!make CUDA_ARCH=\"-gencode arch=compute_75,code=sm_75\" \\\n",
    "      NVCC_FLAGS_COMMON=\"-O3 -Xcompiler -fPIC -Xcompiler -Wall\" \\\n",
    "      fa2\n",
    "\n",
    "print(\"\\nBuild complete!\")\n",
    "!ls -la ../../../lib/*.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comprehensive Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fa2_benchmark.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <chrono>\n",
    "#include <cuda_runtime.h>\n",
    "#include \"flash_attention.h\"\n",
    "#include \"flash_attention_int8.h\"\n",
    "#include \"flash_attention_v2.h\"\n",
    "\n",
    "#define WARMUP 50\n",
    "#define RUNS 500\n",
    "\n",
    "void fill_random(float* data, int size) {\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        data[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "double benchmark_fp32(float* Q, float* K, float* V, float* O,\n",
    "                      int batch_heads, int cache_len, int head_dim) {\n",
    "    // Warmup\n",
    "    for (int i = 0; i < WARMUP; i++) {\n",
    "        flash_attention_decode(Q, K, V, O, batch_heads, cache_len - 1, head_dim);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        flash_attention_decode(Q, K, V, O, batch_heads, cache_len - 1, head_dim);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "    return std::chrono::duration<double, std::milli>(end - start).count() / RUNS;\n",
    "}\n",
    "\n",
    "double benchmark_int8(float* Q, float* K, float* V, float* O,\n",
    "                      int batch_heads, int cache_len, int head_dim) {\n",
    "    for (int i = 0; i < WARMUP; i++) {\n",
    "        flash_attention_int8_decode_fp32(Q, K, V, O, batch_heads, cache_len - 1, head_dim);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        flash_attention_int8_decode_fp32(Q, K, V, O, batch_heads, cache_len - 1, head_dim);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "    return std::chrono::duration<double, std::milli>(end - start).count() / RUNS;\n",
    "}\n",
    "\n",
    "double benchmark_fa2(float* Q, float* K, float* V, float* O,\n",
    "                     int batch_heads, int cache_len, int head_dim) {\n",
    "    for (int i = 0; i < WARMUP; i++) {\n",
    "        flash_attention_v2_decode(Q, K, V, O, batch_heads, cache_len - 1, head_dim);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        flash_attention_v2_decode(Q, K, V, O, batch_heads, cache_len - 1, head_dim);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "\n",
    "    return std::chrono::duration<double, std::milli>(end - start).count() / RUNS;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"\\n\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"\\n\");\n",
    "    printf(\"  FlashAttention-2 Benchmark - SmolLM-135M on T4\\n\");\n",
    "    printf(\"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"=\" \"\\n\\n\");\n",
    "\n",
    "    // SmolLM-135M configuration\n",
    "    int batch_heads = 9, head_dim = 64, num_layers = 9;\n",
    "    int cache_lengths[] = {64, 128, 256, 512};\n",
    "    int num_cache_lengths = 4;\n",
    "\n",
    "    printf(\"Configuration: heads=%d, head_dim=%d, layers=%d\\n\\n\", batch_heads, head_dim, num_layers);\n",
    "\n",
    "    // Initialize all attention implementations\n",
    "    flash_attention_init(1, 9, 2048, head_dim);\n",
    "    flash_attention_init_kv_cache(1, 9, 2048, head_dim);\n",
    "    flash_attention_int8_init(batch_heads, 2048, head_dim);\n",
    "    flash_attention_v2_init(batch_heads, 2048, head_dim);\n",
    "\n",
    "    int single_size = batch_heads * head_dim;\n",
    "    float* Q = (float*)malloc(single_size * sizeof(float));\n",
    "    float* K = (float*)malloc(single_size * sizeof(float));\n",
    "    float* V = (float*)malloc(single_size * sizeof(float));\n",
    "    float* O = (float*)malloc(single_size * sizeof(float));\n",
    "\n",
    "    srand(42);\n",
    "\n",
    "    printf(\"| Cache Len |  FP32 (ms) | INT8 TC (ms) |   FA2 (ms) | Best Speedup |\\n\");\n",
    "    printf(\"|-----------|------------|--------------|------------|--------------|\\n\");\n",
    "\n",
    "    double best_tok = 0;\n",
    "    const char* best_impl = \"\";\n",
    "\n",
    "    for (int c = 0; c < num_cache_lengths; c++) {\n",
    "        int cache_len = cache_lengths[c];\n",
    "\n",
    "        // Fill cache\n",
    "        for (int pos = 0; pos < cache_len; pos++) {\n",
    "            fill_random(K, single_size);\n",
    "            fill_random(V, single_size);\n",
    "            flash_attention_update_kv_cache(K, V, batch_heads, pos, 1, head_dim);\n",
    "            flash_attention_int8_decode_fp32(Q, K, V, O, batch_heads, pos, head_dim);\n",
    "            flash_attention_v2_decode(Q, K, V, O, batch_heads, pos, head_dim);\n",
    "        }\n",
    "        fill_random(Q, single_size);\n",
    "\n",
    "        double fp32_ms = benchmark_fp32(Q, K, V, O, batch_heads, cache_len, head_dim);\n",
    "        double int8_ms = benchmark_int8(Q, K, V, O, batch_heads, cache_len, head_dim);\n",
    "        double fa2_ms = benchmark_fa2(Q, K, V, O, batch_heads, cache_len, head_dim);\n",
    "\n",
    "        double best_ms = fmin(fmin(fp32_ms, int8_ms), fa2_ms);\n",
    "        double speedup = fp32_ms / best_ms;\n",
    "\n",
    "        printf(\"| %9d | %10.4f | %12.4f | %10.4f | %10.2fx  |\\n\",\n",
    "               cache_len, fp32_ms, int8_ms, fa2_ms, speedup);\n",
    "\n",
    "        // Calculate throughput for cache_len=256 (typical)\n",
    "        if (cache_len == 256) {\n",
    "            double fp32_tok = 1000.0 / (fp32_ms * num_layers);\n",
    "            double int8_tok = 1000.0 / (int8_ms * num_layers);\n",
    "            double fa2_tok = 1000.0 / (fa2_ms * num_layers);\n",
    "\n",
    "            if (fp32_tok > best_tok) { best_tok = fp32_tok; best_impl = \"FP32\"; }\n",
    "            if (int8_tok > best_tok) { best_tok = int8_tok; best_impl = \"INT8 TC\"; }\n",
    "            if (fa2_tok > best_tok) { best_tok = fa2_tok; best_impl = \"FA2\"; }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    printf(\"\\n\");\n",
    "    printf(\"Throughput Estimate (cache_len=256, attention only):\\n\");\n",
    "    printf(\"  Best: %.1f tok/s (%s)\\n\", best_tok, best_impl);\n",
    "    printf(\"  Target: 630 tok/s\\n\");\n",
    "    printf(\"  Ollama: 423 tok/s\\n\");\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // JSON output\n",
    "    printf(\"JSON: {\\\"best_tok\\\":%.1f,\\\"best_impl\\\":\\\"%s\\\",\\\"target\\\":630,\\\"ollama\\\":423}\\n\",\n",
    "           best_tok, best_impl);\n",
    "\n",
    "    free(Q); free(K); free(V); free(O);\n",
    "    flash_attention_cleanup();\n",
    "    flash_attention_int8_cleanup();\n",
    "    flash_attention_v2_cleanup();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and run benchmark\n",
    "!nvcc -O3 -gencode arch=compute_75,code=sm_75 --expt-relaxed-constexpr \\\n",
    "    -o fa2_benchmark fa2_benchmark.cu \\\n",
    "    flash_attention.o flash_attention_int8.o flash_attention_v2.o -lcudart\n",
    "!./fa2_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"gpu\": GPU_NAME,\n",
    "    \"compute_capability\": COMPUTE_CAP,\n",
    "    \"benchmark\": \"FlashAttention-2 Comparison\",\n",
    "    \"implementations\": [\"FP32\", \"INT8 Tensor Core\", \"FlashAttention-2\"],\n",
    "    \"target_throughput\": 630,\n",
    "    \"ollama_baseline\": 423\n",
    "}\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f fa2_benchmark fa2_benchmark.cu\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
