{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Kernel Fusion + CUDA Streams Benchmark\n",
    "\n",
    "This notebook tests Phase 2 optimizations:\n",
    "1. **Fused RMSNorm+MatMul kernel** - Combines normalization and matrix multiplication\n",
    "2. **CUDA Streams** - Async H2D/D2H transfers\n",
    "3. **Pinned Memory** - Page-locked host memory for faster transfers\n",
    "\n",
    "**Target**: 400+ tok/s (closing 1.4x gap with Ollama's 423 tok/s)\n",
    "\n",
    "**Requirements:**\n",
    "- NVIDIA GPU (T4, RTX, Jetson)\n",
    "- CUDA Toolkit 11.0+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "import os\n",
    "if not os.path.exists('ollama-api-gateway'):\n",
    "    !git clone https://github.com/umerkhan95/ollama-api-gateway.git\n",
    "else:\n",
    "    print('Repository exists, pulling latest...')\n",
    "    !cd ollama-api-gateway && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CUDA kernels\n",
    "%cd ollama-api-gateway/mojo-gateway/src/kernels\n",
    "!make cuda\n",
    "!ls -la ../../lib/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load CUDA Library with Phase 2 APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load library\n",
    "lib_path = '../../lib/libtmac_kernel_cuda.so'\n",
    "cuda_lib = ctypes.CDLL(lib_path)\n",
    "print(f'Loaded: {lib_path}')\n",
    "\n",
    "# Define function signatures\n",
    "# Basic functions\n",
    "cuda_lib.cuda_available.restype = ctypes.c_int\n",
    "cuda_lib.cuda_device_name.restype = ctypes.c_char_p\n",
    "cuda_lib.cuda_init.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int]\n",
    "cuda_lib.cuda_init.restype = ctypes.c_int\n",
    "cuda_lib.cuda_cleanup.restype = None\n",
    "cuda_lib.cuda_sync.restype = None\n",
    "\n",
    "# Phase 1: Persistent memory API\n",
    "cuda_lib.cuda_load_weights.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_int8),  # weights\n",
    "    ctypes.POINTER(ctypes.c_float),  # scales\n",
    "    ctypes.c_int,  # weight_bytes\n",
    "    ctypes.c_int   # num_rows\n",
    "]\n",
    "cuda_lib.cuda_load_weights.restype = ctypes.c_int\n",
    "cuda_lib.cuda_unload_weights.restype = None\n",
    "cuda_lib.cuda_weights_loaded.restype = ctypes.c_int\n",
    "\n",
    "cuda_lib.tmac_matmul_cuda_persistent.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float),  # activations\n",
    "    ctypes.POINTER(ctypes.c_float),  # output\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int  # M, N, K\n",
    "]\n",
    "cuda_lib.tmac_matmul_cuda_persistent.restype = ctypes.c_int\n",
    "\n",
    "cuda_lib.cuda_load_norm_weights.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float),  # weights\n",
    "    ctypes.c_int  # size\n",
    "]\n",
    "cuda_lib.cuda_load_norm_weights.restype = ctypes.c_int\n",
    "\n",
    "cuda_lib.rmsnorm_cuda_persistent.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float),  # output\n",
    "    ctypes.POINTER(ctypes.c_float),  # input\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_float  # batch_size, size, eps\n",
    "]\n",
    "cuda_lib.rmsnorm_cuda_persistent.restype = ctypes.c_int\n",
    "\n",
    "# Phase 2: Kernel Fusion + Streams API\n",
    "cuda_lib.cuda_init_streams.restype = ctypes.c_int\n",
    "cuda_lib.cuda_cleanup_streams.restype = None\n",
    "cuda_lib.cuda_sync_streams.restype = None\n",
    "\n",
    "cuda_lib.cuda_alloc_pinned.argtypes = [ctypes.c_int, ctypes.c_int]\n",
    "cuda_lib.cuda_alloc_pinned.restype = ctypes.c_int\n",
    "cuda_lib.cuda_free_pinned.restype = None\n",
    "\n",
    "# Fused RMSNorm + MatMul\n",
    "cuda_lib.fused_rmsnorm_matmul_cuda.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float),  # input\n",
    "    ctypes.POINTER(ctypes.c_float),  # output\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int,  # M, N, K\n",
    "    ctypes.c_float  # eps\n",
    "]\n",
    "cuda_lib.fused_rmsnorm_matmul_cuda.restype = ctypes.c_int\n",
    "\n",
    "# Async MatMul\n",
    "cuda_lib.tmac_matmul_cuda_async.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float),  # activations\n",
    "    ctypes.POINTER(ctypes.c_float),  # output\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int  # M, N, K\n",
    "]\n",
    "cuda_lib.tmac_matmul_cuda_async.restype = ctypes.c_int\n",
    "\n",
    "# Fast fused kernel with all optimizations\n",
    "cuda_lib.fused_rmsnorm_matmul_cuda_fast.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float),  # input\n",
    "    ctypes.POINTER(ctypes.c_float),  # output\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int,  # M, N, K\n",
    "    ctypes.c_float  # eps\n",
    "]\n",
    "cuda_lib.fused_rmsnorm_matmul_cuda_fast.restype = ctypes.c_int\n",
    "\n",
    "print('Function signatures defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA and initialize\n",
    "if cuda_lib.cuda_available():\n",
    "    device_name = cuda_lib.cuda_device_name().decode('utf-8')\n",
    "    print(f'CUDA Device: {device_name}')\n",
    "else:\n",
    "    raise RuntimeError('No CUDA device found!')\n",
    "\n",
    "# Initialize with generous buffer sizes\n",
    "max_weights = 100_000_000  # 100MB for weights\n",
    "max_activations = 10_000_000\n",
    "max_output = 10_000_000\n",
    "ret = cuda_lib.cuda_init(max_weights, max_activations, max_output)\n",
    "if ret == 0:\n",
    "    print('CUDA initialized successfully')\n",
    "else:\n",
    "    raise RuntimeError('CUDA initialization failed')\n",
    "\n",
    "# Initialize streams\n",
    "ret = cuda_lib.cuda_init_streams()\n",
    "if ret == 0:\n",
    "    print('CUDA streams initialized')\n",
    "\n",
    "# Allocate pinned memory\n",
    "ret = cuda_lib.cuda_alloc_pinned(max_activations, max_output)\n",
    "if ret == 0:\n",
    "    print('Pinned memory allocated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Parameters (SmolLM-135M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmolLM-135M architecture\n",
    "hidden_size = 576\n",
    "intermediate_size = 1536\n",
    "num_heads = 9\n",
    "head_dim = hidden_size // num_heads  # 64\n",
    "vocab_size = 49152\n",
    "num_layers = 9\n",
    "\n",
    "def calc_weight_bytes(out_features, in_features):\n",
    "    return out_features * ((in_features + 3) // 4)\n",
    "\n",
    "print(f'Hidden size: {hidden_size}')\n",
    "print(f'Intermediate size: {intermediate_size}')\n",
    "print(f'Num layers: {num_layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark: Phase 1 vs Phase 2 APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_phase1_separate(weights, activations, output, scales, norm_weights, M, N, K, iterations=100):\n",
    "    \"\"\"Phase 1: Separate RMSNorm + MatMul calls.\"\"\"\n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    norm_out = np.zeros(K, dtype=np.float32)\n",
    "    norm_out_ptr = norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    # Load weights once\n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        cuda_lib.rmsnorm_cuda_persistent(norm_out_ptr, act_ptr, 1, K, ctypes.c_float(1e-6))\n",
    "        cuda_lib.tmac_matmul_cuda_persistent(norm_out_ptr, out_ptr, M, N, K)\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        cuda_lib.rmsnorm_cuda_persistent(norm_out_ptr, act_ptr, 1, K, ctypes.c_float(1e-6))\n",
    "        cuda_lib.tmac_matmul_cuda_persistent(norm_out_ptr, out_ptr, M, N, K)\n",
    "    cuda_lib.cuda_sync()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    return (end - start) / iterations * 1000  # ms per call\n",
    "\n",
    "def benchmark_phase2_fused(weights, activations, output, scales, norm_weights, M, N, K, iterations=100):\n",
    "    \"\"\"Phase 2: Fused RMSNorm + MatMul kernel.\"\"\"\n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    \n",
    "    # Load weights once\n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        cuda_lib.fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, N, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        cuda_lib.fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, N, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    return (end - start) / iterations * 1000  # ms per call\n",
    "\n",
    "def benchmark_phase2_fast(weights, activations, output, scales, norm_weights, M, N, K, iterations=100):\n",
    "    \"\"\"Phase 2: Fast fused kernel with streams + pinned memory.\"\"\"\n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    \n",
    "    # Load weights once\n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        cuda_lib.fused_rmsnorm_matmul_cuda_fast(act_ptr, out_ptr, M, N, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync_streams()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        cuda_lib.fused_rmsnorm_matmul_cuda_fast(act_ptr, out_ptr, M, N, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync_streams()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    return (end - start) / iterations * 1000  # ms per call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configurations simulating transformer layers\n",
    "test_configs = [\n",
    "    ('QKV Projection', 3 * hidden_size, 1, hidden_size),\n",
    "    ('Output Projection', hidden_size, 1, hidden_size),\n",
    "    ('FFN Up', intermediate_size, 1, hidden_size),\n",
    "    ('FFN Down', hidden_size, 1, intermediate_size),\n",
    "]\n",
    "\n",
    "print('=' * 100)\n",
    "print('PHASE 2 BENCHMARK: Kernel Fusion + CUDA Streams')\n",
    "print('=' * 100)\n",
    "print(f'{\"Layer\":<20} {\"M\":<8} {\"K\":<8} {\"Phase1 (ms)\":<15} {\"Fused (ms)\":<15} {\"Fast (ms)\":<15} {\"Speedup\":<10}')\n",
    "print('-' * 100)\n",
    "\n",
    "results = []\n",
    "iterations = 100\n",
    "\n",
    "for name, M, N, K in test_configs:\n",
    "    # Create test data\n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    weights = np.random.randint(-1, 2, size=weight_bytes, dtype=np.int8)\n",
    "    activations = np.random.randn(K * N).astype(np.float32)\n",
    "    output = np.zeros(M * N, dtype=np.float32)\n",
    "    scales = np.ones(M, dtype=np.float32)\n",
    "    norm_weights = np.ones(K, dtype=np.float32)\n",
    "    \n",
    "    try:\n",
    "        # Benchmark all three approaches\n",
    "        phase1_ms = benchmark_phase1_separate(weights, activations, output, scales, norm_weights, M, N, K, iterations)\n",
    "        fused_ms = benchmark_phase2_fused(weights, activations, output, scales, norm_weights, M, N, K, iterations)\n",
    "        fast_ms = benchmark_phase2_fast(weights, activations, output, scales, norm_weights, M, N, K, iterations)\n",
    "        \n",
    "        speedup = phase1_ms / fast_ms\n",
    "        print(f'{name:<20} {M:<8} {K:<8} {phase1_ms:<15.3f} {fused_ms:<15.3f} {fast_ms:<15.3f} {speedup:<10.2f}x')\n",
    "        results.append((name, M, K, phase1_ms, fused_ms, fast_ms, speedup))\n",
    "    except Exception as e:\n",
    "        print(f'{name:<20} ERROR: {e}')\n",
    "\n",
    "print('-' * 100)\n",
    "\n",
    "if results:\n",
    "    avg_speedup = sum(r[6] for r in results) / len(results)\n",
    "    print(f'\\nAverage Speedup (Phase 1 â†’ Phase 2 Fast): {avg_speedup:.2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Layer Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_layer_phase1(iterations=100):\n",
    "    \"\"\"Simulate one layer with Phase 1 (separate kernels).\"\"\"\n",
    "    # Create weights for QKV and FFN\n",
    "    qkv_weight_bytes = (3 * hidden_size) * ((hidden_size + 3) // 4)\n",
    "    qkv_weights = np.random.randint(-1, 2, size=qkv_weight_bytes, dtype=np.int8)\n",
    "    qkv_scales = np.ones(3 * hidden_size, dtype=np.float32)\n",
    "    \n",
    "    ffn_up_bytes = intermediate_size * ((hidden_size + 3) // 4)\n",
    "    ffn_up_weights = np.random.randint(-1, 2, size=ffn_up_bytes, dtype=np.int8)\n",
    "    ffn_up_scales = np.ones(intermediate_size, dtype=np.float32)\n",
    "    \n",
    "    hidden = np.random.randn(hidden_size).astype(np.float32)\n",
    "    norm_weights = np.ones(hidden_size, dtype=np.float32)\n",
    "    qkv_output = np.zeros(3 * hidden_size, dtype=np.float32)\n",
    "    ffn_output = np.zeros(intermediate_size, dtype=np.float32)\n",
    "    norm_out = np.zeros(hidden_size, dtype=np.float32)\n",
    "    \n",
    "    # Pointers\n",
    "    hidden_ptr = hidden.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_out_ptr = norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    qkv_w_ptr = qkv_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    qkv_s_ptr = qkv_scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    qkv_o_ptr = qkv_output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    ffn_w_ptr = ffn_up_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    ffn_s_ptr = ffn_up_scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    ffn_o_ptr = ffn_output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    # Load QKV weights\n",
    "    cuda_lib.cuda_load_weights(qkv_w_ptr, qkv_s_ptr, qkv_weight_bytes, 3 * hidden_size)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, hidden_size)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        cuda_lib.rmsnorm_cuda_persistent(norm_out_ptr, hidden_ptr, 1, hidden_size, ctypes.c_float(1e-6))\n",
    "        cuda_lib.tmac_matmul_cuda_persistent(norm_out_ptr, qkv_o_ptr, 3*hidden_size, 1, hidden_size)\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    # Benchmark (QKV projection only for fair comparison)\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        cuda_lib.rmsnorm_cuda_persistent(norm_out_ptr, hidden_ptr, 1, hidden_size, ctypes.c_float(1e-6))\n",
    "        cuda_lib.tmac_matmul_cuda_persistent(norm_out_ptr, qkv_o_ptr, 3*hidden_size, 1, hidden_size)\n",
    "    cuda_lib.cuda_sync()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    return (end - start) / iterations * 1000\n",
    "\n",
    "def simulate_layer_phase2(iterations=100):\n",
    "    \"\"\"Simulate one layer with Phase 2 (fused kernel).\"\"\"\n",
    "    qkv_weight_bytes = (3 * hidden_size) * ((hidden_size + 3) // 4)\n",
    "    qkv_weights = np.random.randint(-1, 2, size=qkv_weight_bytes, dtype=np.int8)\n",
    "    qkv_scales = np.ones(3 * hidden_size, dtype=np.float32)\n",
    "    \n",
    "    hidden = np.random.randn(hidden_size).astype(np.float32)\n",
    "    norm_weights = np.ones(hidden_size, dtype=np.float32)\n",
    "    qkv_output = np.zeros(3 * hidden_size, dtype=np.float32)\n",
    "    \n",
    "    hidden_ptr = hidden.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    qkv_w_ptr = qkv_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    qkv_s_ptr = qkv_scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    qkv_o_ptr = qkv_output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    # Load weights\n",
    "    cuda_lib.cuda_load_weights(qkv_w_ptr, qkv_s_ptr, qkv_weight_bytes, 3 * hidden_size)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, hidden_size)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        cuda_lib.fused_rmsnorm_matmul_cuda_fast(hidden_ptr, qkv_o_ptr, 3*hidden_size, 1, hidden_size, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync_streams()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        cuda_lib.fused_rmsnorm_matmul_cuda_fast(hidden_ptr, qkv_o_ptr, 3*hidden_size, 1, hidden_size, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync_streams()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    return (end - start) / iterations * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 60)\n",
    "print('TRANSFORMER LAYER SIMULATION (QKV Projection)')\n",
    "print('=' * 60)\n",
    "\n",
    "phase1_ms = simulate_layer_phase1(iterations=100)\n",
    "phase2_ms = simulate_layer_phase2(iterations=100)\n",
    "\n",
    "print(f'\\nPhase 1 (separate kernels): {phase1_ms:.3f} ms')\n",
    "print(f'Phase 2 (fused + streams):  {phase2_ms:.3f} ms')\n",
    "print(f'Speedup: {phase1_ms / phase2_ms:.2f}x')\n",
    "\n",
    "# Estimate full model throughput\n",
    "print('\\n' + '=' * 60)\n",
    "print('ESTIMATED FULL MODEL THROUGHPUT (SmolLM-135M, 9 layers)')\n",
    "print('=' * 60)\n",
    "\n",
    "# Each layer has multiple operations; estimate based on QKV projection being ~30% of layer time\n",
    "layer_ops_factor = 3  # QKV + O_proj + FFN\n",
    "phase1_layer_ms = phase1_ms * layer_ops_factor\n",
    "phase2_layer_ms = phase2_ms * layer_ops_factor\n",
    "\n",
    "phase1_token_ms = phase1_layer_ms * num_layers\n",
    "phase2_token_ms = phase2_layer_ms * num_layers\n",
    "\n",
    "phase1_tok_s = 1000 / phase1_token_ms\n",
    "phase2_tok_s = 1000 / phase2_token_ms\n",
    "\n",
    "print(f'\\nPhase 1 (Persistent Memory):')\n",
    "print(f'  Per token: {phase1_token_ms:.1f} ms')\n",
    "print(f'  Throughput: {phase1_tok_s:.1f} tok/s')\n",
    "\n",
    "print(f'\\nPhase 2 (Kernel Fusion + Streams):')\n",
    "print(f'  Per token: {phase2_token_ms:.1f} ms')\n",
    "print(f'  Throughput: {phase2_tok_s:.1f} tok/s')\n",
    "\n",
    "print(f'\\nImprovement: {phase2_tok_s / phase1_tok_s:.2f}x faster than Phase 1')\n",
    "print(f'\\nOllama comparison: {phase2_tok_s:.1f} tok/s vs 423 tok/s ({phase2_tok_s/423*100:.1f}% of Ollama)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_lib.cuda_free_pinned()\n",
    "cuda_lib.cuda_cleanup_streams()\n",
    "cuda_lib.cuda_cleanup()\n",
    "print('CUDA resources cleaned up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "**Phase 2 Optimizations:**\n",
    "- Fused RMSNorm + MatMul kernel (50% fewer kernel launches)\n",
    "- CUDA streams for async H2D/D2H transfers\n",
    "- Pinned memory for faster PCIe transfers\n",
    "\n",
    "**Expected Results:**\n",
    "- Phase 1: ~296 tok/s (established baseline)\n",
    "- Phase 2: Target 400+ tok/s\n",
    "- Ollama: 423 tok/s\n",
    "\n",
    "**Next Steps:**\n",
    "- Phase 3: T-MAC tensor core optimization (INT8 WMMA)\n",
    "- Phase 3: CUDA Graphs for full forward pass\n",
    "- Phase 3: Warp-private LUTs (remove atomicAdd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
