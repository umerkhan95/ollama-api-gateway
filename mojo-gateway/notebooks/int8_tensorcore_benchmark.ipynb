{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EdgeLLM INT8 Tensor Core Flash Attention Benchmark\n",
        "\n",
        "This notebook benchmarks our **INT8 Tensor Core** Flash Attention implementation on Tesla T4 GPU.\n",
        "\n",
        "**Key Features:**\n",
        "- INT8 quantized Q, K, V matrices\n",
        "- WMMA (Warp Matrix Multiply Accumulate) for Tensor Core acceleration\n",
        "- ~8x theoretical speedup over FP32\n",
        "\n",
        "**Target:** 630+ tok/s (vs Ollama 423 tok/s baseline)"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup"
      ],
      "metadata": {
        "id": "env_setup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Check GPU\n",
        "result = subprocess.run(['nvidia-smi', '--query-gpu=name,compute_cap,memory.total',\n",
        "                        '--format=csv,noheader'], capture_output=True, text=True)\n",
        "gpu_info = result.stdout.strip()\n",
        "print(f\"GPU: {gpu_info}\")\n",
        "\n",
        "# Parse compute capability\n",
        "parts = gpu_info.split(', ')\n",
        "GPU_NAME = parts[0] if len(parts) > 0 else 'Unknown'\n",
        "COMPUTE_CAP = parts[1] if len(parts) > 1 else '0.0'\n",
        "print(f\"Compute Capability: {COMPUTE_CAP}\")\n",
        "\n",
        "# Verify Tensor Core support (sm_75+)\n",
        "major, minor = COMPUTE_CAP.split('.')\n",
        "if int(major) >= 7 and int(minor) >= 5:\n",
        "    print(\"INT8 Tensor Cores: SUPPORTED\")\n",
        "else:\n",
        "    print(\"WARNING: INT8 Tensor Cores require sm_75+ (Turing or newer)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone repository\n",
        "!rm -rf ollama-api-gateway\n",
        "!git clone --depth 1 https://github.com/umerkhan95/ollama-api-gateway.git\n",
        "%cd ollama-api-gateway/mojo-gateway"
      ],
      "metadata": {
        "id": "clone_repo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Build INT8 Tensor Core Kernels"
      ],
      "metadata": {
        "id": "build_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check CUDA version\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "check_cuda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build INT8 Tensor Core Flash Attention for T4\n",
        "%cd src/kernels/cuda\n",
        "!make clean\n",
        "\n",
        "# Build with T4-specific optimizations (sm_75)\n",
        "!make CUDA_ARCH=\"-gencode arch=compute_75,code=sm_75\" \\\n",
        "      NVCC_FLAGS_COMMON=\"-O3 -Xcompiler -fPIC -Xcompiler -Wall --expt-relaxed-constexpr\" \\\n",
        "      int8\n",
        "\n",
        "# Also build FP32 for comparison\n",
        "!make CUDA_ARCH=\"-gencode arch=compute_75,code=sm_75\" \\\n",
        "      NVCC_FLAGS_COMMON=\"-O3 -Xcompiler -fPIC -Xcompiler -Wall --expt-relaxed-constexpr\" \\\n",
        "      flash\n",
        "\n",
        "print(\"\\nBuild complete!\")\n",
        "!ls -la ../../../lib/*.so 2>/dev/null || echo \"Checking for libraries...\""
      ],
      "metadata": {
        "id": "build_int8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Run INT8 Tests"
      ],
      "metadata": {
        "id": "test_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and run INT8 Flash Attention tests\n",
        "!make CUDA_ARCH=\"-gencode arch=compute_75,code=sm_75\" \\\n",
        "      NVCC_FLAGS_COMMON=\"-O3 -Xcompiler -fPIC -Xcompiler -Wall --expt-relaxed-constexpr\" \\\n",
        "      test-int8"
      ],
      "metadata": {
        "id": "run_int8_tests"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Custom INT8 vs FP32 Benchmark"
      ],
      "metadata": {
        "id": "custom_bench_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile custom_int8_bench.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <chrono>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"flash_attention_int8.h\"\n",
        "#include \"flash_attention.h\"\n",
        "\n",
        "#define WARMUP 20\n",
        "#define RUNS 200\n",
        "\n",
        "void fill_random(float* data, int size) {\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        data[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printf(\"\\n=== INT8 Tensor Core vs FP32 Flash Attention ===\");\n",
        "    printf(\"\\n\\nSmolLM-135M Configuration:\\n\");\n",
        "    printf(\"  Heads: 9, Head dim: 64, Layers: 9\\n\\n\");\n",
        "\n",
        "    int batch_heads = 9, head_dim = 64, num_layers = 9, cache_len = 256;\n",
        "\n",
        "    flash_attention_init(1, 9, 2048, head_dim);\n",
        "    flash_attention_init_kv_cache(1, 9, 2048, head_dim);\n",
        "    flash_attention_int8_init(batch_heads, 2048, head_dim);\n",
        "\n",
        "    int single_size = batch_heads * head_dim;\n",
        "    float* Q = (float*)malloc(single_size * sizeof(float));\n",
        "    float* K = (float*)malloc(single_size * sizeof(float));\n",
        "    float* V = (float*)malloc(single_size * sizeof(float));\n",
        "    float* O = (float*)malloc(single_size * sizeof(float));\n",
        "\n",
        "    srand(42);\n",
        "    for (int pos = 0; pos < cache_len; pos++) {\n",
        "        fill_random(K, single_size);\n",
        "        fill_random(V, single_size);\n",
        "        flash_attention_update_kv_cache(K, V, batch_heads, pos, 1, head_dim);\n",
        "        flash_attention_int8_decode_fp32(Q, K, V, O, batch_heads, pos, head_dim);\n",
        "    }\n",
        "    fill_random(Q, single_size);\n",
        "\n",
        "    // FP32 benchmark\n",
        "    for (int i = 0; i < WARMUP; i++)\n",
        "        flash_attention_decode(Q, K, V, O, batch_heads, cache_len - 1, head_dim);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "    for (int i = 0; i < RUNS; i++)\n",
        "        flash_attention_decode(Q, K, V, O, batch_heads, cache_len - 1, head_dim);\n",
        "    cudaDeviceSynchronize();\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "    double fp32_ms = std::chrono::duration<double, std::milli>(end - start).count() / RUNS;\n",
        "\n",
        "    // INT8 benchmark\n",
        "    for (int i = 0; i < WARMUP; i++)\n",
        "        flash_attention_int8_decode_fp32(Q, K, V, O, batch_heads, cache_len - 1, head_dim);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    start = std::chrono::high_resolution_clock::now();\n",
        "    for (int i = 0; i < RUNS; i++)\n",
        "        flash_attention_int8_decode_fp32(Q, K, V, O, batch_heads, cache_len - 1, head_dim);\n",
        "    cudaDeviceSynchronize();\n",
        "    end = std::chrono::high_resolution_clock::now();\n",
        "    double int8_ms = std::chrono::duration<double, std::milli>(end - start).count() / RUNS;\n",
        "\n",
        "    double speedup = fp32_ms / int8_ms;\n",
        "    printf(\"Results (cache_len=%d):\\n\", cache_len);\n",
        "    printf(\"  FP32 per-layer: %.4f ms\\n\", fp32_ms);\n",
        "    printf(\"  INT8 per-layer: %.4f ms\\n\", int8_ms);\n",
        "    printf(\"  Speedup: %.2fx\\n\\n\", speedup);\n",
        "\n",
        "    double fp32_tok = 1000.0 / (fp32_ms * num_layers / 0.35);\n",
        "    double int8_tok = 1000.0 / (int8_ms * num_layers / 0.35);\n",
        "    printf(\"Estimated throughput:\\n\");\n",
        "    printf(\"  FP32: %.1f tok/s\\n\", fp32_tok);\n",
        "    printf(\"  INT8: %.1f tok/s\\n\", int8_tok);\n",
        "    printf(\"  Target: 630 tok/s, Ollama: 423 tok/s\\n\\n\");\n",
        "\n",
        "    printf(\"JSON: {\\\"fp32_ms\\\":%.4f,\\\"int8_ms\\\":%.4f,\\\"speedup\\\":%.2f,\\\"fp32_tok\\\":%.1f,\\\"int8_tok\\\":%.1f}\\n\",\n",
        "           fp32_ms, int8_ms, speedup, fp32_tok, int8_tok);\n",
        "\n",
        "    free(Q); free(K); free(V); free(O);\n",
        "    flash_attention_cleanup();\n",
        "    flash_attention_int8_cleanup();\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "write_custom_bench"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -gencode arch=compute_75,code=sm_75 --expt-relaxed-constexpr \\\n",
        "    -o custom_int8_bench custom_int8_bench.cu \\\n",
        "    flash_attention_int8.o flash_attention.o -lcudart\n",
        "!./custom_int8_bench"
      ],
      "metadata": {
        "id": "run_custom_bench"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Summary"
      ],
      "metadata": {
        "id": "summary_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "summary = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"gpu\": GPU_NAME,\n",
        "    \"compute_capability\": COMPUTE_CAP,\n",
        "    \"benchmark\": \"INT8 Tensor Core Flash Attention\",\n",
        "    \"target_throughput\": 630,\n",
        "    \"ollama_baseline\": 423\n",
        "}\n",
        "print(json.dumps(summary, indent=2))"
      ],
      "metadata": {
        "id": "generate_summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f custom_int8_bench custom_int8_bench.cu\n",
        "print(\"Cleanup complete!\")"
      ],
      "metadata": {
        "id": "cleanup"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
