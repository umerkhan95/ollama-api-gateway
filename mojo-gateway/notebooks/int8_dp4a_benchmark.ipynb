{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INT8 __dp4a Tensor Core Flash Attention Benchmark\n",
    "\n",
    "Tests the optimized INT8 attention kernel using `__dp4a` intrinsics for 4x throughput.\n",
    "\n",
    "**Optimizations:**\n",
    "- `__dp4a`: 4-element INT8 dot product in single instruction\n",
    "- Vectorized INT8 loads (int8x4 packed as int32)\n",
    "- Async CUDA streams for overlapped copy/compute\n",
    "- Proper warp-level reductions\n",
    "\n",
    "**Expected Results:**\n",
    "- 4x throughput improvement for Q@K^T matmul\n",
    "- Low jitter due to deterministic execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kernel directory\n",
    "!mkdir -p cuda_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda_kernels/flash_attention_int8.h\n",
    "/**\n",
    " * INT8 Tensor Core Flash Attention - Header\n",
    " */\n",
    "#ifndef FLASH_ATTENTION_INT8_H\n",
    "#define FLASH_ATTENTION_INT8_H\n",
    "\n",
    "#include <stdint.h>\n",
    "\n",
    "#ifdef __cplusplus\n",
    "extern \"C\" {\n",
    "#endif\n",
    "\n",
    "int flash_attention_int8_init(int max_batch_heads, int max_cache_len, int head_dim);\n",
    "void flash_attention_int8_cleanup(void);\n",
    "void flash_attention_int8_reset(void);\n",
    "\n",
    "int quantize_tensor_int8(const float* input, int8_t* output, float* scale, int size);\n",
    "\n",
    "int flash_attention_int8_update_cache(\n",
    "    const int8_t* K_int8, const int8_t* V_int8,\n",
    "    float scale_k, float scale_v,\n",
    "    int batch_heads, int cache_pos);\n",
    "\n",
    "int flash_attention_int8_decode(\n",
    "    const int8_t* Q_int8, const int8_t* K_int8, const int8_t* V_int8, float* O,\n",
    "    float scale_q, float scale_k, float scale_v,\n",
    "    int batch_heads, int cache_pos, int head_dim);\n",
    "\n",
    "int flash_attention_int8_decode_fp32(\n",
    "    const float* Q, const float* K_new, const float* V_new, float* O,\n",
    "    int batch_heads, int cache_pos, int head_dim);\n",
    "\n",
    "int flash_attention_int8_decode_gpu(\n",
    "    int batch_heads, int cache_len,\n",
    "    float scale_q, float scale_k, float scale_v);\n",
    "\n",
    "void flash_attention_int8_sync(void);\n",
    "void flash_attention_int8_info(int* initialized, int* max_cache, int* max_bh, int* h_dim);\n",
    "\n",
    "#ifdef __cplusplus\n",
    "}\n",
    "#endif\n",
    "\n",
    "#endif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda_kernels/flash_attention_int8.cu\n",
    "/**\n",
    " * INT8 Tensor Core Flash Attention with __dp4a optimization\n",
    " */\n",
    "#include <cuda_runtime.h>\n",
    "#include <mma.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdio.h>\n",
    "#include <math.h>\n",
    "#include <float.h>\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "#define WMMA_M 16\n",
    "#define WMMA_N 16\n",
    "#define WMMA_K 16\n",
    "#define TC_THREADS 128\n",
    "#define QUANT_SCALE 127.0f\n",
    "\n",
    "#define CUDA_CHECK(call) do { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
    "        return -1; \\\n",
    "    } \\\n",
    "} while(0)\n",
    "\n",
    "// Quantization kernel\n",
    "__global__ void quantize_fp32_to_int8_kernel(\n",
    "    const float* __restrict__ input, int8_t* __restrict__ output,\n",
    "    float* __restrict__ scale, int size\n",
    ") {\n",
    "    __shared__ float s_max;\n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + tid;\n",
    "\n",
    "    float local_max = 0.0f;\n",
    "    for (int i = idx; i < size; i += gridDim.x * blockDim.x) {\n",
    "        local_max = fmaxf(local_max, fabsf(input[i]));\n",
    "    }\n",
    "\n",
    "    __shared__ float shared_max[256];\n",
    "    shared_max[tid] = local_max;\n",
    "    __syncthreads();\n",
    "\n",
    "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) shared_max[tid] = fmaxf(shared_max[tid], shared_max[tid + s]);\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (tid == 0) atomicMax((int*)&s_max, __float_as_int(shared_max[0]));\n",
    "    __syncthreads();\n",
    "\n",
    "    float max_val = s_max;\n",
    "    float quant_scale = (max_val > 0.0f) ? (QUANT_SCALE / max_val) : 1.0f;\n",
    "\n",
    "    if (tid == 0 && blockIdx.x == 0) *scale = max_val / QUANT_SCALE;\n",
    "\n",
    "    for (int i = idx; i < size; i += gridDim.x * blockDim.x) {\n",
    "        float val = input[i] * quant_scale;\n",
    "        val = fminf(fmaxf(val, -127.0f), 127.0f);\n",
    "        output[i] = (int8_t)rintf(val);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized __dp4a INT8 attention kernel\n",
    "__global__ void flash_attention_int8_decode_dp4a_kernel(\n",
    "    const int8_t* __restrict__ Q_int8,\n",
    "    const int8_t* __restrict__ K_cache_int8,\n",
    "    const int8_t* __restrict__ V_cache_int8,\n",
    "    float* __restrict__ O,\n",
    "    const float scale_q, const float scale_k, const float scale_v,\n",
    "    const int cache_len, const int head_dim, const float attn_scale\n",
    ") {\n",
    "    const int batch_head_idx = blockIdx.x;\n",
    "    const int tid = threadIdx.x;\n",
    "    const int warp_id = tid / 32;\n",
    "    const int lane_id = tid % 32;\n",
    "    const int num_warps = TC_THREADS / 32;\n",
    "\n",
    "    extern __shared__ char shared_mem[];\n",
    "    float* s_scores = (float*)shared_mem;\n",
    "    int* s_Q_packed = (int*)(s_scores + ((cache_len + 15) / 16) * 16);\n",
    "\n",
    "    const int8_t* Q_ptr = Q_int8 + batch_head_idx * head_dim;\n",
    "    const int8_t* K_ptr = K_cache_int8 + batch_head_idx * cache_len * head_dim;\n",
    "    const int8_t* V_ptr = V_cache_int8 + batch_head_idx * cache_len * head_dim;\n",
    "    float* O_ptr = O + batch_head_idx * head_dim;\n",
    "\n",
    "    const int packed_dim = head_dim / 4;\n",
    "    const int* Q_packed = (const int*)Q_ptr;\n",
    "    for (int d = tid; d < packed_dim; d += TC_THREADS) {\n",
    "        s_Q_packed[d] = Q_packed[d];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    // Phase 1: Q @ K^T using __dp4a\n",
    "    float local_max = -FLT_MAX;\n",
    "\n",
    "    for (int k_idx = tid; k_idx < cache_len; k_idx += TC_THREADS) {\n",
    "        const int* K_row_packed = (const int*)(K_ptr + k_idx * head_dim);\n",
    "        int32_t dot = 0;\n",
    "\n",
    "        #pragma unroll 4\n",
    "        for (int d = 0; d < packed_dim; d++) {\n",
    "            dot = __dp4a(s_Q_packed[d], K_row_packed[d], dot);\n",
    "        }\n",
    "\n",
    "        float score = (float)dot * scale_q * scale_k * attn_scale;\n",
    "        s_scores[k_idx] = score;\n",
    "        local_max = fmaxf(local_max, score);\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    // Warp reduction for max\n",
    "    #pragma unroll\n",
    "    for (int offset = 16; offset > 0; offset /= 2) {\n",
    "        local_max = fmaxf(local_max, __shfl_down_sync(0xffffffff, local_max, offset));\n",
    "    }\n",
    "\n",
    "    __shared__ float s_block_max[4];\n",
    "    if (lane_id == 0) s_block_max[warp_id] = local_max;\n",
    "    __syncthreads();\n",
    "\n",
    "    if (tid == 0) {\n",
    "        float block_max = s_block_max[0];\n",
    "        for (int i = 1; i < num_warps; i++) block_max = fmaxf(block_max, s_block_max[i]);\n",
    "        s_block_max[0] = block_max;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    float global_max = s_block_max[0];\n",
    "\n",
    "    // Softmax\n",
    "    float local_sum = 0.0f;\n",
    "    for (int k_idx = tid; k_idx < cache_len; k_idx += TC_THREADS) {\n",
    "        float exp_score = expf(s_scores[k_idx] - global_max);\n",
    "        s_scores[k_idx] = exp_score;\n",
    "        local_sum += exp_score;\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    #pragma unroll\n",
    "    for (int offset = 16; offset > 0; offset /= 2) {\n",
    "        local_sum += __shfl_down_sync(0xffffffff, local_sum, offset);\n",
    "    }\n",
    "\n",
    "    __shared__ float s_block_sum[4];\n",
    "    if (lane_id == 0) s_block_sum[warp_id] = local_sum;\n",
    "    __syncthreads();\n",
    "\n",
    "    if (tid == 0) {\n",
    "        float total_sum = 0.0f;\n",
    "        for (int i = 0; i < num_warps; i++) total_sum += s_block_sum[i];\n",
    "        s_block_sum[0] = total_sum;\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    float inv_sum = 1.0f / s_block_sum[0];\n",
    "    for (int k_idx = tid; k_idx < cache_len; k_idx += TC_THREADS) {\n",
    "        s_scores[k_idx] *= inv_sum;\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    // Phase 2: softmax @ V\n",
    "    for (int d = tid; d < head_dim; d += TC_THREADS) {\n",
    "        float acc = 0.0f;\n",
    "        int k_idx = 0;\n",
    "\n",
    "        for (; k_idx + 3 < cache_len; k_idx += 4) {\n",
    "            float s0 = s_scores[k_idx];\n",
    "            float s1 = s_scores[k_idx + 1];\n",
    "            float s2 = s_scores[k_idx + 2];\n",
    "            float s3 = s_scores[k_idx + 3];\n",
    "\n",
    "            acc += s0 * (float)V_ptr[(k_idx + 0) * head_dim + d];\n",
    "            acc += s1 * (float)V_ptr[(k_idx + 1) * head_dim + d];\n",
    "            acc += s2 * (float)V_ptr[(k_idx + 2) * head_dim + d];\n",
    "            acc += s3 * (float)V_ptr[(k_idx + 3) * head_dim + d];\n",
    "        }\n",
    "\n",
    "        for (; k_idx < cache_len; k_idx++) {\n",
    "            acc += s_scores[k_idx] * (float)V_ptr[k_idx * head_dim + d];\n",
    "        }\n",
    "\n",
    "        O_ptr[d] = acc * scale_v;\n",
    "    }\n",
    "}\n",
    "\n",
    "// C Interface\n",
    "extern \"C\" {\n",
    "\n",
    "static int8_t* d_Q_int8 = nullptr;\n",
    "static cudaStream_t int8_compute_stream = nullptr;\n",
    "static cudaStream_t int8_copy_stream = nullptr;\n",
    "static int8_t* d_K_cache_int8 = nullptr;\n",
    "static int8_t* d_V_cache_int8 = nullptr;\n",
    "static float* d_O_float = nullptr;\n",
    "static float* d_scales = nullptr;\n",
    "static int int8_initialized = 0;\n",
    "static int int8_max_cache_len = 0;\n",
    "static int int8_max_batch_heads = 0;\n",
    "static int int8_head_dim = 0;\n",
    "\n",
    "void flash_attention_int8_cleanup(void);\n",
    "\n",
    "int flash_attention_int8_init(int max_batch_heads, int max_cache_len, int head_dim) {\n",
    "    if (int8_initialized) flash_attention_int8_cleanup();\n",
    "\n",
    "    if (head_dim % 4 != 0) {\n",
    "        fprintf(stderr, \"head_dim must be multiple of 4 for __dp4a\\n\");\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    int8_max_batch_heads = max_batch_heads;\n",
    "    int8_max_cache_len = max_cache_len;\n",
    "    int8_head_dim = head_dim;\n",
    "\n",
    "    size_t q_size = max_batch_heads * head_dim * sizeof(int8_t);\n",
    "    size_t cache_size = max_batch_heads * max_cache_len * head_dim * sizeof(int8_t);\n",
    "    size_t output_size = max_batch_heads * head_dim * sizeof(float);\n",
    "\n",
    "    CUDA_CHECK(cudaMalloc(&d_Q_int8, q_size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_K_cache_int8, cache_size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_V_cache_int8, cache_size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_O_float, output_size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_scales, 3 * sizeof(float)));\n",
    "\n",
    "    CUDA_CHECK(cudaStreamCreate(&int8_compute_stream));\n",
    "    CUDA_CHECK(cudaStreamCreate(&int8_copy_stream));\n",
    "\n",
    "    float default_scales[3] = {1.0f/127.0f, 1.0f/127.0f, 1.0f/127.0f};\n",
    "    CUDA_CHECK(cudaMemcpy(d_scales, default_scales, 3*sizeof(float), cudaMemcpyHostToDevice));\n",
    "\n",
    "    int8_initialized = 1;\n",
    "\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"INT8 __dp4a Flash Attention initialized\\n\");\n",
    "    printf(\"  GPU: %s (SM %d.%d)\\n\", prop.name, prop.major, prop.minor);\n",
    "    printf(\"  Config: batch_heads=%d, max_cache=%d, head_dim=%d\\n\",\n",
    "           max_batch_heads, max_cache_len, head_dim);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void flash_attention_int8_cleanup(void) {\n",
    "    if (d_Q_int8) { cudaFree(d_Q_int8); d_Q_int8 = nullptr; }\n",
    "    if (d_K_cache_int8) { cudaFree(d_K_cache_int8); d_K_cache_int8 = nullptr; }\n",
    "    if (d_V_cache_int8) { cudaFree(d_V_cache_int8); d_V_cache_int8 = nullptr; }\n",
    "    if (d_O_float) { cudaFree(d_O_float); d_O_float = nullptr; }\n",
    "    if (d_scales) { cudaFree(d_scales); d_scales = nullptr; }\n",
    "    if (int8_compute_stream) { cudaStreamDestroy(int8_compute_stream); int8_compute_stream = nullptr; }\n",
    "    if (int8_copy_stream) { cudaStreamDestroy(int8_copy_stream); int8_copy_stream = nullptr; }\n",
    "    int8_initialized = 0;\n",
    "}\n",
    "\n",
    "void flash_attention_int8_reset(void) {}\n",
    "\n",
    "int quantize_tensor_int8(const float* input, int8_t* output, float* scale, int size) {\n",
    "    float* d_input;\n",
    "    float* d_scale;\n",
    "    int8_t* d_output;\n",
    "\n",
    "    CUDA_CHECK(cudaMalloc(&d_input, size * sizeof(float)));\n",
    "    CUDA_CHECK(cudaMalloc(&d_output, size * sizeof(int8_t)));\n",
    "    CUDA_CHECK(cudaMalloc(&d_scale, sizeof(float)));\n",
    "\n",
    "    CUDA_CHECK(cudaMemcpy(d_input, input, size * sizeof(float), cudaMemcpyHostToDevice));\n",
    "\n",
    "    dim3 block(256);\n",
    "    dim3 grid((size + 255) / 256);\n",
    "    quantize_fp32_to_int8_kernel<<<grid, block>>>(d_input, d_output, d_scale, size);\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "\n",
    "    CUDA_CHECK(cudaMemcpy(output, d_output, size * sizeof(int8_t), cudaMemcpyDeviceToHost));\n",
    "    CUDA_CHECK(cudaMemcpy(scale, d_scale, sizeof(float), cudaMemcpyDeviceToHost));\n",
    "\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_scale);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "int flash_attention_int8_update_cache(\n",
    "    const int8_t* K_int8, const int8_t* V_int8,\n",
    "    float scale_k, float scale_v,\n",
    "    int batch_heads, int cache_pos\n",
    ") {\n",
    "    if (!int8_initialized) return -1;\n",
    "\n",
    "    for (int bh = 0; bh < batch_heads; bh++) {\n",
    "        size_t cache_offset = (bh * int8_max_cache_len + cache_pos) * int8_head_dim;\n",
    "        size_t src_offset = bh * int8_head_dim;\n",
    "\n",
    "        CUDA_CHECK(cudaMemcpyAsync(d_K_cache_int8 + cache_offset, K_int8 + src_offset,\n",
    "                                   int8_head_dim * sizeof(int8_t), cudaMemcpyHostToDevice, int8_copy_stream));\n",
    "        CUDA_CHECK(cudaMemcpyAsync(d_V_cache_int8 + cache_offset, V_int8 + src_offset,\n",
    "                                   int8_head_dim * sizeof(int8_t), cudaMemcpyHostToDevice, int8_copy_stream));\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "int flash_attention_int8_decode(\n",
    "    const int8_t* Q_int8, const int8_t* K_int8, const int8_t* V_int8, float* O,\n",
    "    float scale_q, float scale_k, float scale_v,\n",
    "    int batch_heads, int cache_pos, int head_dim\n",
    ") {\n",
    "    if (!int8_initialized) return -1;\n",
    "\n",
    "    CUDA_CHECK(cudaMemcpyAsync(d_Q_int8, Q_int8, batch_heads * head_dim * sizeof(int8_t),\n",
    "                               cudaMemcpyHostToDevice, int8_copy_stream));\n",
    "\n",
    "    flash_attention_int8_update_cache(K_int8, V_int8, scale_k, scale_v, batch_heads, cache_pos);\n",
    "    CUDA_CHECK(cudaStreamSynchronize(int8_copy_stream));\n",
    "\n",
    "    int cache_len = cache_pos + 1;\n",
    "    float attn_scale = 1.0f / sqrtf((float)head_dim);\n",
    "\n",
    "    size_t scores_size = ((cache_len + 15) / 16 * 16) * sizeof(float);\n",
    "    size_t q_packed_size = ((head_dim / 4) + 3) / 4 * 4 * sizeof(int);\n",
    "    size_t smem_size = scores_size + q_packed_size;\n",
    "\n",
    "    dim3 grid(batch_heads);\n",
    "    dim3 block(TC_THREADS);\n",
    "\n",
    "    flash_attention_int8_decode_dp4a_kernel<<<grid, block, smem_size, int8_compute_stream>>>(\n",
    "        d_Q_int8, d_K_cache_int8, d_V_cache_int8, d_O_float,\n",
    "        scale_q, scale_k, scale_v, cache_len, head_dim, attn_scale);\n",
    "\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    CUDA_CHECK(cudaStreamSynchronize(int8_compute_stream));\n",
    "\n",
    "    CUDA_CHECK(cudaMemcpy(O, d_O_float, batch_heads * head_dim * sizeof(float), cudaMemcpyDeviceToHost));\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "int flash_attention_int8_decode_fp32(\n",
    "    const float* Q, const float* K_new, const float* V_new, float* O,\n",
    "    int batch_heads, int cache_pos, int head_dim\n",
    ") {\n",
    "    if (!int8_initialized) return -1;\n",
    "\n",
    "    int single_size = batch_heads * head_dim;\n",
    "\n",
    "    int8_t* h_Q_int8 = (int8_t*)malloc(single_size);\n",
    "    int8_t* h_K_int8 = (int8_t*)malloc(single_size);\n",
    "    int8_t* h_V_int8 = (int8_t*)malloc(single_size);\n",
    "    float scale_q, scale_k, scale_v;\n",
    "\n",
    "    quantize_tensor_int8(Q, h_Q_int8, &scale_q, single_size);\n",
    "    quantize_tensor_int8(K_new, h_K_int8, &scale_k, single_size);\n",
    "    quantize_tensor_int8(V_new, h_V_int8, &scale_v, single_size);\n",
    "\n",
    "    int ret = flash_attention_int8_decode(\n",
    "        h_Q_int8, h_K_int8, h_V_int8, O,\n",
    "        scale_q, scale_k, scale_v,\n",
    "        batch_heads, cache_pos, head_dim);\n",
    "\n",
    "    free(h_Q_int8);\n",
    "    free(h_K_int8);\n",
    "    free(h_V_int8);\n",
    "\n",
    "    return ret;\n",
    "}\n",
    "\n",
    "int flash_attention_int8_decode_gpu(int batch_heads, int cache_len, float scale_q, float scale_k, float scale_v) {\n",
    "    if (!int8_initialized) return -1;\n",
    "\n",
    "    float attn_scale = 1.0f / sqrtf((float)int8_head_dim);\n",
    "    size_t scores_size = ((cache_len + 15) / 16 * 16) * sizeof(float);\n",
    "    size_t q_packed_size = ((int8_head_dim / 4) + 3) / 4 * 4 * sizeof(int);\n",
    "    size_t smem_size = scores_size + q_packed_size;\n",
    "\n",
    "    dim3 grid(batch_heads);\n",
    "    dim3 block(TC_THREADS);\n",
    "\n",
    "    flash_attention_int8_decode_dp4a_kernel<<<grid, block, smem_size, int8_compute_stream>>>(\n",
    "        d_Q_int8, d_K_cache_int8, d_V_cache_int8, d_O_float,\n",
    "        scale_q, scale_k, scale_v, cache_len, int8_head_dim, attn_scale);\n",
    "\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void flash_attention_int8_sync(void) {\n",
    "    if (int8_compute_stream) cudaStreamSynchronize(int8_compute_stream);\n",
    "}\n",
    "\n",
    "void flash_attention_int8_info(int* initialized, int* max_cache, int* max_bh, int* h_dim) {\n",
    "    if (initialized) *initialized = int8_initialized;\n",
    "    if (max_cache) *max_cache = int8_max_cache_len;\n",
    "    if (max_bh) *max_bh = int8_max_batch_heads;\n",
    "    if (h_dim) *h_dim = int8_head_dim;\n",
    "}\n",
    "\n",
    "} // extern \"C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda_kernels/benchmark.cu\n",
    "/**\n",
    " * INT8 __dp4a Flash Attention Benchmark\n",
    " */\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <chrono>\n",
    "#include <algorithm>\n",
    "#include <cuda_runtime.h>\n",
    "#include \"flash_attention_int8.h\"\n",
    "\n",
    "#define WARMUP_RUNS 50\n",
    "#define BENCHMARK_RUNS 200\n",
    "\n",
    "void fill_random(float* data, int size) {\n",
    "    for (int i = 0; i < size; i++)\n",
    "        data[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"\\n=== INT8 __dp4a Flash Attention Benchmark ===\\n\\n\");\n",
    "\n",
    "    int device_count;\n",
    "    cudaGetDeviceCount(&device_count);\n",
    "    if (device_count == 0) {\n",
    "        printf(\"ERROR: No CUDA GPUs found\\n\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    printf(\"GPU: %s (SM %d.%d)\\n\", prop.name, prop.major, prop.minor);\n",
    "    printf(\"Memory: %.2f GB\\n\\n\", prop.totalGlobalMem / 1e9);\n",
    "\n",
    "    // Test configurations\n",
    "    struct Config {\n",
    "        const char* name;\n",
    "        int num_heads;\n",
    "        int head_dim;\n",
    "        int num_layers;\n",
    "    };\n",
    "\n",
    "    Config configs[] = {\n",
    "        {\"SmolLM-135M\", 9, 64, 9},\n",
    "        {\"Qwen-0.5B\", 14, 64, 24},\n",
    "        {\"Qwen-1.5B\", 12, 128, 28}\n",
    "    };\n",
    "\n",
    "    for (auto& cfg : configs) {\n",
    "        printf(\"\\n=== %s ===\\n\", cfg.name);\n",
    "        printf(\"  Heads: %d, Head dim: %d, Layers: %d\\n\", cfg.num_heads, cfg.head_dim, cfg.num_layers);\n",
    "\n",
    "        int batch_heads = cfg.num_heads;\n",
    "        int cache_len = 256;\n",
    "\n",
    "        flash_attention_int8_init(batch_heads, 2048, cfg.head_dim);\n",
    "\n",
    "        int single_size = batch_heads * cfg.head_dim;\n",
    "        float* Q = (float*)malloc(single_size * sizeof(float));\n",
    "        float* K = (float*)malloc(single_size * sizeof(float));\n",
    "        float* V = (float*)malloc(single_size * sizeof(float));\n",
    "        float* O = (float*)malloc(single_size * sizeof(float));\n",
    "\n",
    "        srand(42);\n",
    "\n",
    "        // Fill cache\n",
    "        for (int pos = 0; pos < cache_len; pos++) {\n",
    "            fill_random(K, single_size);\n",
    "            fill_random(V, single_size);\n",
    "            flash_attention_int8_decode_fp32(Q, K, V, O, batch_heads, pos, cfg.head_dim);\n",
    "        }\n",
    "\n",
    "        fill_random(Q, single_size);\n",
    "        fill_random(K, single_size);\n",
    "        fill_random(V, single_size);\n",
    "\n",
    "        // Warmup\n",
    "        printf(\"  Warmup: %d runs...\\n\", WARMUP_RUNS);\n",
    "        for (int i = 0; i < WARMUP_RUNS; i++) {\n",
    "            flash_attention_int8_decode_fp32(Q, K, V, O, batch_heads, cache_len - 1, cfg.head_dim);\n",
    "        }\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        // Benchmark\n",
    "        printf(\"  Benchmark: %d runs...\\n\", BENCHMARK_RUNS);\n",
    "        double* latencies = (double*)malloc(BENCHMARK_RUNS * sizeof(double));\n",
    "\n",
    "        for (int i = 0; i < BENCHMARK_RUNS; i++) {\n",
    "            auto start = std::chrono::high_resolution_clock::now();\n",
    "            flash_attention_int8_decode_fp32(Q, K, V, O, batch_heads, cache_len - 1, cfg.head_dim);\n",
    "            cudaDeviceSynchronize();\n",
    "            auto end = std::chrono::high_resolution_clock::now();\n",
    "            latencies[i] = std::chrono::duration<double, std::micro>(end - start).count();\n",
    "        }\n",
    "\n",
    "        // Sort for percentiles\n",
    "        std::sort(latencies, latencies + BENCHMARK_RUNS);\n",
    "\n",
    "        double min_us = latencies[0];\n",
    "        double max_us = latencies[BENCHMARK_RUNS - 1];\n",
    "        double median_us = latencies[BENCHMARK_RUNS / 2];\n",
    "        double p95_us = latencies[(int)(BENCHMARK_RUNS * 0.95)];\n",
    "        double p99_us = latencies[(int)(BENCHMARK_RUNS * 0.99)];\n",
    "\n",
    "        double mean_us = 0;\n",
    "        for (int i = 0; i < BENCHMARK_RUNS; i++) mean_us += latencies[i];\n",
    "        mean_us /= BENCHMARK_RUNS;\n",
    "\n",
    "        double variance = 0;\n",
    "        for (int i = 0; i < BENCHMARK_RUNS; i++) {\n",
    "            double diff = latencies[i] - mean_us;\n",
    "            variance += diff * diff;\n",
    "        }\n",
    "        double std_us = sqrt(variance / BENCHMARK_RUNS);\n",
    "\n",
    "        double median_ms = median_us / 1000.0;\n",
    "        double per_token_attn_ms = median_ms * cfg.num_layers;\n",
    "        double attn_throughput = 1000.0 / per_token_attn_ms;\n",
    "        double estimated_throughput = 1000.0 / (per_token_attn_ms / 0.35);\n",
    "\n",
    "        printf(\"\\n  Results:\\n\");\n",
    "        printf(\"    Layer latency (median): %.2f us (%.4f ms)\\n\", median_us, median_ms);\n",
    "        printf(\"    Layer latency (mean):   %.2f +/- %.2f us\\n\", mean_us, std_us);\n",
    "        printf(\"    Layer latency (P95):    %.2f us\\n\", p95_us);\n",
    "        printf(\"    Layer latency (P99):    %.2f us\\n\", p99_us);\n",
    "        printf(\"    Jitter (std/median):    %.2f%%\\n\", (std_us / median_us) * 100);\n",
    "        printf(\"\\n\");\n",
    "        printf(\"    Per-token attention:    %.3f ms\\n\", per_token_attn_ms);\n",
    "        printf(\"    Attn-only throughput:   %.1f tok/s\\n\", attn_throughput);\n",
    "        printf(\"    Est. total throughput:  %.1f tok/s\\n\", estimated_throughput);\n",
    "\n",
    "        printf(\"\\n  JSON:\\n\");\n",
    "        printf(\"  {\\n\");\n",
    "        printf(\"    \\\"model\\\": \\\"%s\\\",\\n\", cfg.name);\n",
    "        printf(\"    \\\"kernel\\\": \\\"INT8_dp4a\\\",\\n\");\n",
    "        printf(\"    \\\"layer_latency_us\\\": %.2f,\\n\", median_us);\n",
    "        printf(\"    \\\"std_us\\\": %.2f,\\n\", std_us);\n",
    "        printf(\"    \\\"p95_us\\\": %.2f,\\n\", p95_us);\n",
    "        printf(\"    \\\"p99_us\\\": %.2f,\\n\", p99_us);\n",
    "        printf(\"    \\\"attn_throughput\\\": %.1f,\\n\", attn_throughput);\n",
    "        printf(\"    \\\"estimated_throughput\\\": %.1f\\n\", estimated_throughput);\n",
    "        printf(\"  }\\n\");\n",
    "\n",
    "        free(Q);\n",
    "        free(K);\n",
    "        free(V);\n",
    "        free(O);\n",
    "        free(latencies);\n",
    "        flash_attention_int8_cleanup();\n",
    "    }\n",
    "\n",
    "    printf(\"\\n=== Benchmark Complete ===\\n\");\n",
    "    printf(\"\\nTargets: 630 tok/s (EdgeLLM), Baseline: 423 tok/s (Ollama)\\n\");\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "!nvcc -O3 -gencode arch=compute_75,code=sm_75 \\\n",
    "     --expt-relaxed-constexpr \\\n",
    "     -I cuda_kernels \\\n",
    "     cuda_kernels/flash_attention_int8.cu cuda_kernels/benchmark.cu \\\n",
    "     -o benchmark_int8_dp4a \\\n",
    "     -lcudart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "!./benchmark_int8_dp4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The `__dp4a` intrinsic provides 4x throughput for INT8 dot products by computing:\n",
    "```\n",
    "d = c + a[0]*b[0] + a[1]*b[1] + a[2]*b[2] + a[3]*b[3]\n",
    "```\n",
    "where `a` and `b` are packed int8x4 values.\n",
    "\n",
    "**Expected improvements:**\n",
    "- 4x faster Q@K^T matmul\n",
    "- Lower memory bandwidth (INT8 vs FP32/FP16)\n",
    "- Better cache utilization due to smaller data types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
