{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EdgeLLM Comprehensive Benchmark Suite\n",
    "\n",
    "This notebook collects **all** performance metrics for rigorous comparison against other inference engines.\n",
    "\n",
    "## Metrics Collected\n",
    "\n",
    "| Category | Metrics |\n",
    "|----------|--------|\n",
    "| **GPU Hardware** | Model, Driver, CUDA, Power (avg/peak), VRAM |\n",
    "| **Memory** | VRAM used, Host RAM, PCIe transfers |\n",
    "| **Kernel Stats** | Launches, Execution time, SM occupancy |\n",
    "| **Latency** | Mean, P50, P90, P95, P99, Max, Jitter |\n",
    "| **Throughput Scaling** | 1k, 4k, 8k, 16k context |\n",
    "| **CPU Overhead** | Utilization, Time per token, Threads |\n",
    "| **Reproducibility** | OS, Compiler, Build flags |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pynvml psutil numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import psutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    NVML_AVAILABLE = True\n",
    "except:\n",
    "    NVML_AVAILABLE = False\n",
    "    print('Warning: pynvml not available, some GPU stats will be limited')\n",
    "\n",
    "print(f'Setup complete. NVML available: {NVML_AVAILABLE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and build\n",
    "import os\n",
    "if not os.path.exists('ollama-api-gateway'):\n",
    "    !git clone https://github.com/umerkhan95/ollama-api-gateway.git\n",
    "else:\n",
    "    print('Repository exists, pulling latest...')\n",
    "    !cd ollama-api-gateway && git pull\n",
    "\n",
    "%cd ollama-api-gateway/mojo-gateway/src/kernels\n",
    "!make cuda\n",
    "!ls -la ../../lib/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Hardware Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_info():\n",
    "    \"\"\"Collect detailed GPU hardware information.\"\"\"\n",
    "    info = {}\n",
    "    \n",
    "    # Basic info from nvidia-smi\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=name,driver_version,memory.total,power.limit,power.max_limit,clocks.max.sm,clocks.max.mem,pcie.link.gen.current,pcie.link.width.current',\n",
    "             '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        parts = result.stdout.strip().split(', ')\n",
    "        info['gpu_model'] = parts[0]\n",
    "        info['driver_version'] = parts[1]\n",
    "        info['vram_total_mb'] = int(parts[2])\n",
    "        info['power_limit_w'] = float(parts[3])\n",
    "        info['power_max_w'] = float(parts[4])\n",
    "        info['max_sm_clock_mhz'] = int(parts[5])\n",
    "        info['max_mem_clock_mhz'] = int(parts[6])\n",
    "        info['pcie_gen'] = parts[7]\n",
    "        info['pcie_width'] = parts[8]\n",
    "    except Exception as e:\n",
    "        print(f'nvidia-smi error: {e}')\n",
    "    \n",
    "    # CUDA version\n",
    "    try:\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'release' in line:\n",
    "                info['cuda_version'] = line.split('release')[1].split(',')[0].strip()\n",
    "    except:\n",
    "        info['cuda_version'] = 'unknown'\n",
    "    \n",
    "    # Detailed info from NVML\n",
    "    if NVML_AVAILABLE:\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        info['gpu_uuid'] = pynvml.nvmlDeviceGetUUID(handle)\n",
    "        info['compute_capability'] = '.'.join(map(str, pynvml.nvmlDeviceGetCudaComputeCapability(handle)))\n",
    "        info['sm_count'] = pynvml.nvmlDeviceGetNumGpuCores(handle) if hasattr(pynvml, 'nvmlDeviceGetNumGpuCores') else 'N/A'\n",
    "        \n",
    "        # Memory info\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        info['vram_used_mb'] = mem_info.used // (1024 * 1024)\n",
    "        info['vram_free_mb'] = mem_info.free // (1024 * 1024)\n",
    "        \n",
    "        # Power info\n",
    "        try:\n",
    "            info['current_power_w'] = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0\n",
    "        except:\n",
    "            info['current_power_w'] = 'N/A'\n",
    "    \n",
    "    return info\n",
    "\n",
    "gpu_info = get_gpu_info()\n",
    "print('='*60)\n",
    "print('GPU HARDWARE INFO')\n",
    "print('='*60)\n",
    "for k, v in gpu_info.items():\n",
    "    print(f'{k:25}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reproducibility Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reproducibility_info():\n",
    "    \"\"\"Collect system info for reproducibility.\"\"\"\n",
    "    info = {}\n",
    "    \n",
    "    # OS info\n",
    "    info['os'] = platform.system()\n",
    "    info['os_version'] = platform.release()\n",
    "    info['os_detail'] = platform.platform()\n",
    "    \n",
    "    # CPU info\n",
    "    info['cpu_model'] = platform.processor()\n",
    "    info['cpu_cores'] = psutil.cpu_count(logical=False)\n",
    "    info['cpu_threads'] = psutil.cpu_count(logical=True)\n",
    "    info['ram_total_gb'] = round(psutil.virtual_memory().total / (1024**3), 2)\n",
    "    \n",
    "    # Compiler info\n",
    "    try:\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        info['nvcc_version'] = result.stdout.strip().split('\\n')[-1]\n",
    "    except:\n",
    "        info['nvcc_version'] = 'unknown'\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(['gcc', '--version'], capture_output=True, text=True)\n",
    "        info['gcc_version'] = result.stdout.strip().split('\\n')[0]\n",
    "    except:\n",
    "        info['gcc_version'] = 'unknown'\n",
    "    \n",
    "    # Build flags (from Makefile)\n",
    "    info['cuda_build_flags'] = '-O3 -Xcompiler -fPIC -gencode arch=compute_75,code=sm_75'\n",
    "    \n",
    "    # Power management\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '-q', '-d', 'POWER'], capture_output=True, text=True)\n",
    "        if 'Persistence Mode' in result.stdout:\n",
    "            info['persistence_mode'] = 'Enabled' if 'Enabled' in result.stdout.split('Persistence Mode')[1].split('\\n')[0] else 'Disabled'\n",
    "    except:\n",
    "        info['persistence_mode'] = 'unknown'\n",
    "    \n",
    "    info['clock_locking'] = 'No'  # Default for cloud GPUs\n",
    "    info['timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    return info\n",
    "\n",
    "repro_info = get_reproducibility_info()\n",
    "print('='*60)\n",
    "print('REPRODUCIBILITY INFO')\n",
    "print('='*60)\n",
    "for k, v in repro_info.items():\n",
    "    print(f'{k:25}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CUDA Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "lib_path = '../../lib/libtmac_kernel_cuda.so'\n",
    "cuda_lib = ctypes.CDLL(lib_path)\n",
    "print(f'Loaded: {lib_path}')\n",
    "\n",
    "# Basic functions\n",
    "cuda_lib.cuda_available.restype = ctypes.c_int\n",
    "cuda_lib.cuda_device_name.restype = ctypes.c_char_p\n",
    "cuda_lib.cuda_init.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int]\n",
    "cuda_lib.cuda_init.restype = ctypes.c_int\n",
    "cuda_lib.cuda_cleanup.restype = None\n",
    "cuda_lib.cuda_sync.restype = None\n",
    "\n",
    "# Phase 1: Persistent memory API\n",
    "cuda_lib.cuda_load_weights.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_int8), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int\n",
    "]\n",
    "cuda_lib.cuda_load_weights.restype = ctypes.c_int\n",
    "cuda_lib.cuda_unload_weights.restype = None\n",
    "cuda_lib.cuda_load_norm_weights.argtypes = [ctypes.POINTER(ctypes.c_float), ctypes.c_int]\n",
    "cuda_lib.cuda_load_norm_weights.restype = ctypes.c_int\n",
    "\n",
    "# Phase 2.1: Optimized kernels\n",
    "cuda_lib.streaming_fused_rmsnorm_matmul_cuda.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
    "]\n",
    "cuda_lib.streaming_fused_rmsnorm_matmul_cuda.restype = ctypes.c_int\n",
    "\n",
    "cuda_lib.fused_rmsnorm_matmul_cuda_adaptive.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
    "]\n",
    "cuda_lib.fused_rmsnorm_matmul_cuda_adaptive.restype = ctypes.c_int\n",
    "\n",
    "print('Function signatures defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CUDA\n",
    "if cuda_lib.cuda_available():\n",
    "    device_name = cuda_lib.cuda_device_name().decode('utf-8')\n",
    "    print(f'CUDA Device: {device_name}')\n",
    "else:\n",
    "    raise RuntimeError('No CUDA device found!')\n",
    "\n",
    "max_weights = 100_000_000\n",
    "max_activations = 10_000_000\n",
    "max_output = 10_000_000\n",
    "ret = cuda_lib.cuda_init(max_weights, max_activations, max_output)\n",
    "if ret == 0:\n",
    "    print('CUDA initialized successfully')\n",
    "else:\n",
    "    raise RuntimeError('CUDA initialization failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Workload Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmolLM-135M architecture\n",
    "WORKLOAD = {\n",
    "    'model_name': 'SmolLM-135M',\n",
    "    'hidden_size': 576,\n",
    "    'intermediate_size': 1536,\n",
    "    'num_heads': 9,\n",
    "    'head_dim': 64,\n",
    "    'vocab_size': 49152,\n",
    "    'num_layers': 9,\n",
    "    'quantization': 'BitNet 1.58-bit',\n",
    "    'batch_size': 1,\n",
    "    'sampling_method': 'greedy',\n",
    "    'temperature': 0.0,\n",
    "    'kv_cache_enabled': False,  # For this benchmark\n",
    "    'kv_cache_size_mb': 0,\n",
    "}\n",
    "\n",
    "print('='*60)\n",
    "print('WORKLOAD DEFINITION')\n",
    "print('='*60)\n",
    "for k, v in WORKLOAD.items():\n",
    "    print(f'{k:25}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory & Transfer Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transfer_stats(M, N, K):\n",
    "    \"\"\"Calculate PCIe transfer bytes per token.\"\"\"\n",
    "    # Host -> Device transfers per token\n",
    "    activation_bytes = K * N * 4  # float32\n",
    "    \n",
    "    # Device -> Host transfers per token  \n",
    "    output_bytes = M * N * 4  # float32\n",
    "    \n",
    "    # Weights are persistent (transferred once, not per token)\n",
    "    weight_bytes = M * ((K + 3) // 4)  # packed int8 ternary\n",
    "    scale_bytes = M * 4  # float32\n",
    "    norm_weight_bytes = K * 4  # float32\n",
    "    \n",
    "    return {\n",
    "        'pcie_h2d_per_token_bytes': activation_bytes,\n",
    "        'pcie_d2h_per_token_bytes': output_bytes,\n",
    "        'weight_bytes_one_time': weight_bytes + scale_bytes + norm_weight_bytes,\n",
    "    }\n",
    "\n",
    "# Calculate for each layer type\n",
    "hidden = WORKLOAD['hidden_size']\n",
    "intermediate = WORKLOAD['intermediate_size']\n",
    "\n",
    "layer_configs = [\n",
    "    ('QKV Projection', 3 * hidden, 1, hidden),\n",
    "    ('Output Projection', hidden, 1, hidden),\n",
    "    ('FFN Up', intermediate, 1, hidden),\n",
    "    ('FFN Down', hidden, 1, intermediate),\n",
    "]\n",
    "\n",
    "print('='*60)\n",
    "print('MEMORY & TRANSFER STATS (per layer)')\n",
    "print('='*60)\n",
    "total_h2d = 0\n",
    "total_d2h = 0\n",
    "for name, M, N, K in layer_configs:\n",
    "    stats = calculate_transfer_stats(M, N, K)\n",
    "    total_h2d += stats['pcie_h2d_per_token_bytes']\n",
    "    total_d2h += stats['pcie_d2h_per_token_bytes']\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  H→D: {stats['pcie_h2d_per_token_bytes']:,} bytes\")\n",
    "    print(f\"  D→H: {stats['pcie_d2h_per_token_bytes']:,} bytes\")\n",
    "\n",
    "print(f\"\\nTotal per token (all layers x {WORKLOAD['num_layers']}):\")\n",
    "print(f\"  H→D: {total_h2d * WORKLOAD['num_layers']:,} bytes\")\n",
    "print(f\"  D→H: {total_d2h * WORKLOAD['num_layers']:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Latency Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_latency_detailed(cuda_lib, M, K, iterations=1000):\n",
    "    \"\"\"Collect detailed latency statistics.\"\"\"\n",
    "    N = 1  # batch size\n",
    "    \n",
    "    # Setup\n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    weights = np.random.randint(-1, 2, size=weight_bytes, dtype=np.int8)\n",
    "    activations = np.random.randn(K * N).astype(np.float32)\n",
    "    output = np.zeros(M * N, dtype=np.float32)\n",
    "    scales = np.ones(M, dtype=np.float32)\n",
    "    norm_weights = np.ones(K, dtype=np.float32)\n",
    "    \n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    # Load weights\n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(50):\n",
    "        cuda_lib.streaming_fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    # Collect individual latencies\n",
    "    latencies = []\n",
    "    for _ in range(iterations):\n",
    "        start = time.perf_counter()\n",
    "        cuda_lib.streaming_fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, K, ctypes.c_float(1e-6))\n",
    "        cuda_lib.cuda_sync()\n",
    "        end = time.perf_counter()\n",
    "        latencies.append((end - start) * 1000)  # ms\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    \n",
    "    latencies = np.array(latencies)\n",
    "    \n",
    "    # Calculate stats\n",
    "    stats = {\n",
    "        'mean_ms': np.mean(latencies),\n",
    "        'std_ms': np.std(latencies),\n",
    "        'min_ms': np.min(latencies),\n",
    "        'max_ms': np.max(latencies),\n",
    "        'p50_ms': np.percentile(latencies, 50),\n",
    "        'p90_ms': np.percentile(latencies, 90),\n",
    "        'p95_ms': np.percentile(latencies, 95),\n",
    "        'p99_ms': np.percentile(latencies, 99),\n",
    "        'variance_ms2': np.var(latencies),\n",
    "        'iterations': iterations,\n",
    "    }\n",
    "    \n",
    "    # Calculate jitter (gap variance)\n",
    "    gaps = np.diff(latencies)\n",
    "    stats['gap_variance_ms2'] = np.var(gaps)\n",
    "    stats['longest_stall_ms'] = np.max(gaps) if len(gaps) > 0 else 0\n",
    "    \n",
    "    return stats, latencies\n",
    "\n",
    "print('Running detailed latency benchmark...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark each layer type\n",
    "latency_results = {}\n",
    "\n",
    "print('='*80)\n",
    "print('LATENCY STATS (1000 iterations per layer)')\n",
    "print('='*80)\n",
    "print(f'{\"Layer\":<20} {\"Mean\":>8} {\"P50\":>8} {\"P90\":>8} {\"P95\":>8} {\"P99\":>8} {\"Max\":>8} {\"Jitter\":>10}')\n",
    "print(f'{\"\":<20} {\"(ms)\":>8} {\"(ms)\":>8} {\"(ms)\":>8} {\"(ms)\":>8} {\"(ms)\":>8} {\"(ms)\":>8} {\"(ms²)\":>10}')\n",
    "print('-'*80)\n",
    "\n",
    "for name, M, N, K in layer_configs:\n",
    "    stats, raw_latencies = benchmark_latency_detailed(cuda_lib, M, K, iterations=1000)\n",
    "    latency_results[name] = stats\n",
    "    print(f'{name:<20} {stats[\"mean_ms\"]:>8.4f} {stats[\"p50_ms\"]:>8.4f} {stats[\"p90_ms\"]:>8.4f} '\n",
    "          f'{stats[\"p95_ms\"]:>8.4f} {stats[\"p99_ms\"]:>8.4f} {stats[\"max_ms\"]:>8.4f} {stats[\"gap_variance_ms2\"]:>10.6f}')\n",
    "\n",
    "print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Power Monitoring During Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_with_power(cuda_lib, M, K, iterations=500):\n",
    "    \"\"\"Benchmark with power monitoring.\"\"\"\n",
    "    N = 1\n",
    "    \n",
    "    # Setup\n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    weights = np.random.randint(-1, 2, size=weight_bytes, dtype=np.int8)\n",
    "    activations = np.random.randn(K * N).astype(np.float32)\n",
    "    output = np.zeros(M * N, dtype=np.float32)\n",
    "    scales = np.ones(M, dtype=np.float32)\n",
    "    norm_weights = np.ones(K, dtype=np.float32)\n",
    "    \n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(50):\n",
    "        cuda_lib.streaming_fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    power_readings = []\n",
    "    latencies = []\n",
    "    \n",
    "    if NVML_AVAILABLE:\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    \n",
    "    # Benchmark with power sampling\n",
    "    for _ in range(iterations):\n",
    "        if NVML_AVAILABLE:\n",
    "            try:\n",
    "                power_readings.append(pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        cuda_lib.streaming_fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, K, ctypes.c_float(1e-6))\n",
    "        cuda_lib.cuda_sync()\n",
    "        end = time.perf_counter()\n",
    "        latencies.append((end - start) * 1000)\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    \n",
    "    results = {\n",
    "        'avg_latency_ms': np.mean(latencies),\n",
    "        'throughput_ops_per_sec': 1000 / np.mean(latencies),\n",
    "    }\n",
    "    \n",
    "    if power_readings:\n",
    "        results['avg_power_w'] = np.mean(power_readings)\n",
    "        results['peak_power_w'] = np.max(power_readings)\n",
    "        results['min_power_w'] = np.min(power_readings)\n",
    "        # Energy per operation\n",
    "        results['energy_per_op_j'] = results['avg_power_w'] * results['avg_latency_ms'] / 1000\n",
    "    \n",
    "    return results\n",
    "\n",
    "print('Running power benchmark...')\n",
    "power_results = {}\n",
    "\n",
    "print('='*80)\n",
    "print('POWER STATS')\n",
    "print('='*80)\n",
    "\n",
    "for name, M, N, K in layer_configs:\n",
    "    results = benchmark_with_power(cuda_lib, M, K, iterations=500)\n",
    "    power_results[name] = results\n",
    "    print(f'\\n{name}:')\n",
    "    for k, v in results.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f'  {k}: {v:.4f}')\n",
    "        else:\n",
    "            print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Full Token Generation Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_token_generation(cuda_lib, num_tokens=100, num_layers=9):\n",
    "    \"\"\"Simulate full token generation through all layers.\"\"\"\n",
    "    hidden = WORKLOAD['hidden_size']\n",
    "    intermediate = WORKLOAD['intermediate_size']\n",
    "    \n",
    "    # Pre-allocate all layer weights and buffers\n",
    "    layers = [\n",
    "        ('QKV', 3 * hidden, hidden),\n",
    "        ('Out', hidden, hidden),\n",
    "        ('FFN_Up', intermediate, hidden),\n",
    "        ('FFN_Down', hidden, intermediate),\n",
    "    ]\n",
    "    \n",
    "    # Prepare weights for each layer\n",
    "    layer_data = []\n",
    "    for name, M, K in layers:\n",
    "        weight_bytes = M * ((K + 3) // 4)\n",
    "        weights = np.random.randint(-1, 2, size=weight_bytes, dtype=np.int8)\n",
    "        scales = np.ones(M, dtype=np.float32)\n",
    "        norm_weights = np.ones(K, dtype=np.float32)\n",
    "        layer_data.append((name, M, K, weights, scales, norm_weights))\n",
    "    \n",
    "    token_latencies = []\n",
    "    power_readings = []\n",
    "    \n",
    "    if NVML_AVAILABLE:\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    \n",
    "    # Generate tokens\n",
    "    for token_idx in range(num_tokens):\n",
    "        token_start = time.perf_counter()\n",
    "        \n",
    "        # Process through all transformer layers\n",
    "        for layer_idx in range(num_layers):\n",
    "            for name, M, K, weights, scales, norm_weights in layer_data:\n",
    "                activations = np.random.randn(K).astype(np.float32)\n",
    "                output = np.zeros(M, dtype=np.float32)\n",
    "                \n",
    "                weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "                act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "                out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "                scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "                norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "                \n",
    "                weight_bytes = M * ((K + 3) // 4)\n",
    "                cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "                cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "                cuda_lib.streaming_fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, K, ctypes.c_float(1e-6))\n",
    "                cuda_lib.cuda_unload_weights()\n",
    "        \n",
    "        cuda_lib.cuda_sync()\n",
    "        token_end = time.perf_counter()\n",
    "        token_latencies.append((token_end - token_start) * 1000)\n",
    "        \n",
    "        if NVML_AVAILABLE:\n",
    "            try:\n",
    "                power_readings.append(pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    token_latencies = np.array(token_latencies)\n",
    "    \n",
    "    results = {\n",
    "        'num_tokens': num_tokens,\n",
    "        'num_layers': num_layers,\n",
    "        'mean_token_latency_ms': np.mean(token_latencies),\n",
    "        'p50_token_latency_ms': np.percentile(token_latencies, 50),\n",
    "        'p90_token_latency_ms': np.percentile(token_latencies, 90),\n",
    "        'p95_token_latency_ms': np.percentile(token_latencies, 95),\n",
    "        'p99_token_latency_ms': np.percentile(token_latencies, 99),\n",
    "        'max_token_latency_ms': np.max(token_latencies),\n",
    "        'token_gap_variance_ms2': np.var(np.diff(token_latencies)),\n",
    "        'throughput_tok_s': 1000 / np.mean(token_latencies),\n",
    "    }\n",
    "    \n",
    "    if power_readings:\n",
    "        results['avg_power_w'] = np.mean(power_readings)\n",
    "        results['peak_power_w'] = np.max(power_readings)\n",
    "        results['energy_per_token_j'] = results['avg_power_w'] * results['mean_token_latency_ms'] / 1000\n",
    "    \n",
    "    return results, token_latencies\n",
    "\n",
    "print('Simulating full token generation (100 tokens)...')\n",
    "token_gen_results, token_latencies_raw = simulate_token_generation(cuda_lib, num_tokens=100)\n",
    "\n",
    "print('='*80)\n",
    "print('TOKEN GENERATION STATS (Full Model Simulation)')\n",
    "print('='*80)\n",
    "for k, v in token_gen_results.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f'{k:30}: {v:.4f}')\n",
    "    else:\n",
    "        print(f'{k:30}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Kernel Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_stats():\n",
    "    \"\"\"Calculate kernel launch stats per token.\"\"\"\n",
    "    num_layers = WORKLOAD['num_layers']\n",
    "    \n",
    "    # Per layer:\n",
    "    # - QKV projection: 1 kernel (streaming fused)\n",
    "    # - Output projection: 1 kernel\n",
    "    # - FFN Up: 1 kernel\n",
    "    # - FFN Down: 1 kernel\n",
    "    # Total per layer: 4 kernels\n",
    "    \n",
    "    kernels_per_layer = 4\n",
    "    total_kernels_per_token = kernels_per_layer * num_layers\n",
    "    \n",
    "    # Average kernel execution time from latency results\n",
    "    avg_kernel_time = np.mean([latency_results[name]['mean_ms'] for name in latency_results])\n",
    "    \n",
    "    return {\n",
    "        'kernels_per_layer': kernels_per_layer,\n",
    "        'total_kernels_per_token': total_kernels_per_token,\n",
    "        'avg_kernel_exec_time_ms': avg_kernel_time,\n",
    "    }\n",
    "\n",
    "kernel_stats = get_kernel_stats()\n",
    "print('='*60)\n",
    "print('KERNEL STATS')\n",
    "print('='*60)\n",
    "for k, v in kernel_stats.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f'{k:30}: {v:.4f}')\n",
    "    else:\n",
    "        print(f'{k:30}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. GPU Utilization & Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_utilization():\n",
    "    \"\"\"Get GPU utilization metrics.\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    if NVML_AVAILABLE:\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        \n",
    "        # Utilization\n",
    "        try:\n",
    "            util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            stats['gpu_utilization_pct'] = util.gpu\n",
    "            stats['memory_utilization_pct'] = util.memory\n",
    "        except:\n",
    "            stats['gpu_utilization_pct'] = 'N/A'\n",
    "            stats['memory_utilization_pct'] = 'N/A'\n",
    "        \n",
    "        # Memory\n",
    "        mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        stats['vram_used_mb'] = mem.used // (1024 * 1024)\n",
    "        stats['vram_total_mb'] = mem.total // (1024 * 1024)\n",
    "        stats['vram_free_mb'] = mem.free // (1024 * 1024)\n",
    "    \n",
    "    # Host RAM\n",
    "    ram = psutil.virtual_memory()\n",
    "    stats['host_ram_used_mb'] = ram.used // (1024 * 1024)\n",
    "    stats['host_ram_total_mb'] = ram.total // (1024 * 1024)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "utilization_stats = get_gpu_utilization()\n",
    "print('='*60)\n",
    "print('GPU UTILIZATION & MEMORY')\n",
    "print('='*60)\n",
    "for k, v in utilization_stats.items():\n",
    "    print(f'{k:30}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. CPU Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_cpu_overhead():\n",
    "    \"\"\"Measure CPU overhead during GPU inference.\"\"\"\n",
    "    import threading\n",
    "    \n",
    "    cpu_samples = []\n",
    "    stop_sampling = False\n",
    "    \n",
    "    def sample_cpu():\n",
    "        while not stop_sampling:\n",
    "            cpu_samples.append(psutil.cpu_percent(interval=0.01))\n",
    "    \n",
    "    # Start CPU sampling thread\n",
    "    sampler = threading.Thread(target=sample_cpu)\n",
    "    sampler.start()\n",
    "    \n",
    "    # Run inference\n",
    "    M, K = 3 * WORKLOAD['hidden_size'], WORKLOAD['hidden_size']\n",
    "    N = 1\n",
    "    \n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    weights = np.random.randint(-1, 2, size=weight_bytes, dtype=np.int8)\n",
    "    activations = np.random.randn(K * N).astype(np.float32)\n",
    "    output = np.zeros(M * N, dtype=np.float32)\n",
    "    scales = np.ones(M, dtype=np.float32)\n",
    "    norm_weights = np.ones(K, dtype=np.float32)\n",
    "    \n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        cuda_lib.streaming_fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    \n",
    "    # Stop sampling\n",
    "    stop_sampling = True\n",
    "    sampler.join()\n",
    "    \n",
    "    return {\n",
    "        'cpu_utilization_pct': np.mean(cpu_samples) if cpu_samples else 0,\n",
    "        'cpu_peak_pct': np.max(cpu_samples) if cpu_samples else 0,\n",
    "        'threads_used': psutil.cpu_count(logical=True),\n",
    "        'host_sync_calls_per_token': 1,  # One cuda_sync per token\n",
    "    }\n",
    "\n",
    "cpu_overhead = measure_cpu_overhead()\n",
    "print('='*60)\n",
    "print('CPU OVERHEAD')\n",
    "print('='*60)\n",
    "for k, v in cpu_overhead.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f'{k:30}: {v:.2f}')\n",
    "    else:\n",
    "        print(f'{k:30}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 12.5 Throughput Scaling Tests\n\nTest throughput at different context lengths (1k, 4k, 8k, 16k tokens).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def benchmark_throughput_scaling(cuda_lib, context_lengths=[1024, 4096, 8192, 16384]):\n    \"\"\"Benchmark throughput at different context lengths.\n    \n    Note: For single token generation (batch=1), context length affects\n    KV cache size but not the kernel performance directly. This simulates\n    the memory pressure from larger contexts.\n    \"\"\"\n    results = {}\n    hidden = WORKLOAD['hidden_size']\n    \n    for ctx_len in context_lengths:\n        # Simulate memory pressure from KV cache\n        # KV cache size = 2 * num_layers * ctx_len * hidden_size * sizeof(float16)\n        kv_cache_size_mb = 2 * WORKLOAD['num_layers'] * ctx_len * hidden * 2 / (1024 * 1024)\n        \n        # For generation phase, we process one token at a time\n        # The key metric is: does having a larger KV cache in memory affect performance?\n        M, K = 3 * hidden, hidden  # QKV projection (largest layer)\n        N = 1\n        \n        weight_bytes = M * ((K + 3) // 4)\n        weights = np.random.randint(-1, 2, size=weight_bytes, dtype=np.int8)\n        activations = np.random.randn(K * N).astype(np.float32)\n        output = np.zeros(M * N, dtype=np.float32)\n        scales = np.ones(M, dtype=np.float32)\n        norm_weights = np.ones(K, dtype=np.float32)\n        \n        # Simulate KV cache memory allocation\n        if ctx_len <= 8192:  # Don't OOM on small GPUs\n            try:\n                kv_cache_k = np.zeros((WORKLOAD['num_layers'], ctx_len, hidden), dtype=np.float16)\n                kv_cache_v = np.zeros((WORKLOAD['num_layers'], ctx_len, hidden), dtype=np.float16)\n            except MemoryError:\n                kv_cache_k = None\n                kv_cache_v = None\n        else:\n            kv_cache_k = None\n            kv_cache_v = None\n        \n        weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n        act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n        out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n        scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n        norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n        \n        cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n        cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n        \n        # Warmup\n        for _ in range(50):\n            cuda_lib.streaming_fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, K, ctypes.c_float(1e-6))\n        cuda_lib.cuda_sync()\n        \n        # Benchmark\n        iterations = 500\n        start = time.perf_counter()\n        for _ in range(iterations):\n            cuda_lib.streaming_fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, K, ctypes.c_float(1e-6))\n        cuda_lib.cuda_sync()\n        elapsed = time.perf_counter() - start\n        \n        cuda_lib.cuda_unload_weights()\n        \n        # Free KV cache\n        del kv_cache_k, kv_cache_v\n        \n        avg_latency_ms = (elapsed / iterations) * 1000\n        throughput = 1000 / avg_latency_ms\n        \n        results[ctx_len] = {\n            'context_length': ctx_len,\n            'kv_cache_size_mb': kv_cache_size_mb,\n            'avg_latency_ms': avg_latency_ms,\n            'throughput_tok_s': throughput,\n        }\n    \n    # Calculate degradation slope\n    if len(results) >= 2:\n        ctx_lens = sorted(results.keys())\n        throughputs = [results[c]['throughput_tok_s'] for c in ctx_lens]\n        # Degradation per 1k tokens\n        base_throughput = throughputs[0]\n        degradations = [(base_throughput - t) / base_throughput * 100 for t in throughputs]\n        ctx_diffs = [(c - ctx_lens[0]) / 1000 for c in ctx_lens]\n        if ctx_diffs[-1] > 0:\n            slope = degradations[-1] / ctx_diffs[-1]\n        else:\n            slope = 0\n    else:\n        slope = 0\n    \n    return results, slope\n\nprint('Running throughput scaling tests...')\nscaling_results, degradation_slope = benchmark_throughput_scaling(cuda_lib)\n\nprint('='*80)\nprint('THROUGHPUT SCALING')\nprint('='*80)\nprint(f'{\"Context\":>10} {\"KV Cache\":>12} {\"Latency\":>12} {\"Throughput\":>12}')\nprint(f'{\"(tokens)\":>10} {\"(MB)\":>12} {\"(ms)\":>12} {\"(tok/s)\":>12}')\nprint('-'*50)\n\nfor ctx_len in sorted(scaling_results.keys()):\n    r = scaling_results[ctx_len]\n    print(f'{ctx_len:>10} {r[\"kv_cache_size_mb\"]:>12.2f} {r[\"avg_latency_ms\"]:>12.4f} {r[\"throughput_tok_s\"]:>12.2f}')\n\nprint(f'\\nDegradation slope: {degradation_slope:.4f}% per 1k tokens')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Compile Full Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compile all results into final report\nfull_report = {\n    'timestamp': datetime.now().isoformat(),\n    'benchmark_version': '1.0.0',\n    \n    # GPU Hardware\n    'gpu_model': gpu_info.get('gpu_model', 'N/A'),\n    'driver_version': gpu_info.get('driver_version', 'N/A'),\n    'cuda_version': gpu_info.get('cuda_version', 'N/A'),\n    'compute_capability': gpu_info.get('compute_capability', 'N/A'),\n    'vram_total_mb': gpu_info.get('vram_total_mb', 0),\n    \n    # Power\n    'power_limit_w': gpu_info.get('power_limit_w', 0),\n    'avg_power_w': token_gen_results.get('avg_power_w', 'N/A'),\n    'peak_power_w': token_gen_results.get('peak_power_w', 'N/A'),\n    'energy_per_token_j': token_gen_results.get('energy_per_token_j', 'N/A'),\n    \n    # Memory\n    'vram_used_mb': utilization_stats.get('vram_used_mb', 0),\n    'host_ram_used_mb': utilization_stats.get('host_ram_used_mb', 0),\n    \n    # PCIe Transfer Stats\n    'pcie_h2d_per_token_bytes': total_h2d * WORKLOAD['num_layers'],\n    'pcie_d2h_per_token_bytes': total_d2h * WORKLOAD['num_layers'],\n    \n    # Latency\n    'mean_token_latency_ms': token_gen_results['mean_token_latency_ms'],\n    'p50_token_latency_ms': token_gen_results['p50_token_latency_ms'],\n    'p90_token_latency_ms': token_gen_results['p90_token_latency_ms'],\n    'p95_token_latency_ms': token_gen_results['p95_token_latency_ms'],\n    'p99_token_latency_ms': token_gen_results['p99_token_latency_ms'],\n    'max_token_latency_ms': token_gen_results['max_token_latency_ms'],\n    'token_gap_variance_ms2': token_gen_results['token_gap_variance_ms2'],\n    \n    # Throughput\n    'throughput_tok_s': token_gen_results['throughput_tok_s'],\n    \n    # Throughput Scaling\n    'throughput_1k_ctx_tok_s': scaling_results.get(1024, {}).get('throughput_tok_s', 'N/A'),\n    'throughput_4k_ctx_tok_s': scaling_results.get(4096, {}).get('throughput_tok_s', 'N/A'),\n    'throughput_8k_ctx_tok_s': scaling_results.get(8192, {}).get('throughput_tok_s', 'N/A'),\n    'throughput_16k_ctx_tok_s': scaling_results.get(16384, {}).get('throughput_tok_s', 'N/A'),\n    'throughput_degradation_slope_pct_per_1k': degradation_slope,\n    \n    # Kernel Stats\n    'kernels_per_token': kernel_stats['total_kernels_per_token'],\n    'avg_kernel_exec_time_ms': kernel_stats['avg_kernel_exec_time_ms'],\n    \n    # GPU Utilization\n    'gpu_utilization_pct': utilization_stats.get('gpu_utilization_pct', 'N/A'),\n    \n    # CPU Overhead\n    'cpu_utilization_pct': cpu_overhead['cpu_utilization_pct'],\n    'cpu_peak_pct': cpu_overhead['cpu_peak_pct'],\n    'threads_used': cpu_overhead['threads_used'],\n    'host_sync_calls_per_token': cpu_overhead['host_sync_calls_per_token'],\n    \n    # Workload\n    'model_name': WORKLOAD['model_name'],\n    'batch_size': WORKLOAD['batch_size'],\n    'num_layers': WORKLOAD['num_layers'],\n    'hidden_size': WORKLOAD['hidden_size'],\n    'quantization': WORKLOAD['quantization'],\n    'sampling_method': WORKLOAD['sampling_method'],\n    'temperature': WORKLOAD['temperature'],\n    'kv_cache_enabled': WORKLOAD['kv_cache_enabled'],\n    \n    # Reproducibility\n    'os': repro_info['os'],\n    'os_version': repro_info['os_version'],\n    'nvcc_version': repro_info['nvcc_version'],\n    'gcc_version': repro_info['gcc_version'],\n    'build_flags': repro_info['cuda_build_flags'],\n    'clock_locking': repro_info['clock_locking'],\n    'persistence_mode': repro_info.get('persistence_mode', 'N/A'),\n}\n\nprint('='*80)\nprint('FULL BENCHMARK REPORT')\nprint('='*80)\nfor k, v in full_report.items():\n    if isinstance(v, float):\n        print(f'{k:40}: {v:.4f}')\n    else:\n        print(f'{k:40}: {v}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save report to JSON\n",
    "report_filename = f'benchmark_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(report_filename, 'w') as f:\n",
    "    json.dump(full_report, f, indent=2, default=str)\n",
    "print(f'\\nReport saved to: {report_filename}')\n",
    "\n",
    "# Also print as formatted table for easy copying\n",
    "print('\\n' + '='*80)\n",
    "print('MARKDOWN TABLE FORMAT')\n",
    "print('='*80)\n",
    "print('| Metric | Value |')\n",
    "print('|--------|-------|')\n",
    "for k, v in full_report.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f'| {k} | {v:.4f} |')\n",
    "    else:\n",
    "        print(f'| {k} | {v} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_lib.cuda_cleanup()\n",
    "if NVML_AVAILABLE:\n",
    "    pynvml.nvmlShutdown()\n",
    "print('Resources cleaned up')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}