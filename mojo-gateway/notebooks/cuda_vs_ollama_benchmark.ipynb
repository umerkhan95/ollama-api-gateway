{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EdgeLLM CUDA vs Ollama Benchmark\n",
        "\n",
        "Comprehensive throughput comparison between EdgeLLM CUDA kernels and Ollama.\n",
        "\n",
        "**Goal**: Demonstrate higher tok/s with our CUDA T-MAC kernels.\n",
        "\n",
        "**Hardware**: Tesla T4 (15GB VRAM, 40 SMs, Compute 7.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone and build\n",
        "import os\n",
        "if not os.path.exists('ollama-api-gateway'):\n",
        "    !git clone https://github.com/umerkhan95/ollama-api-gateway.git\n",
        "else:\n",
        "    !cd ollama-api-gateway && git pull\n",
        "\n",
        "%cd ollama-api-gateway/mojo-gateway/src/kernels\n",
        "!make cuda\n",
        "!ls -la ../../lib/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ollama for comparison\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Ollama server in background\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start Ollama serve\n",
        "ollama_process = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "time.sleep(5)\n",
        "print('Ollama server started')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull a small model for testing\n",
        "!ollama pull smollm:135m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load CUDA Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ctypes\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Load CUDA library\n",
        "cuda_lib = ctypes.CDLL('../../lib/libtmac_kernel_cuda.so')\n",
        "\n",
        "# Define function signatures\n",
        "cuda_lib.cuda_available.restype = ctypes.c_int\n",
        "cuda_lib.cuda_device_name.restype = ctypes.c_char_p\n",
        "cuda_lib.cuda_init.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int]\n",
        "cuda_lib.cuda_init.restype = ctypes.c_int\n",
        "cuda_lib.cuda_cleanup.restype = None\n",
        "cuda_lib.cuda_sync.restype = None\n",
        "\n",
        "cuda_lib.tmac_matmul_cuda.argtypes = [\n",
        "    ctypes.POINTER(ctypes.c_int8),   # weights\n",
        "    ctypes.POINTER(ctypes.c_float),  # activations\n",
        "    ctypes.POINTER(ctypes.c_float),  # output\n",
        "    ctypes.POINTER(ctypes.c_float),  # scales\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_int  # M, N, K\n",
        "]\n",
        "cuda_lib.tmac_matmul_cuda.restype = ctypes.c_int\n",
        "\n",
        "cuda_lib.rmsnorm_cuda.argtypes = [\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
        "]\n",
        "cuda_lib.rmsnorm_cuda.restype = ctypes.c_int\n",
        "\n",
        "cuda_lib.softmax_cuda.argtypes = [\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.POINTER(ctypes.c_float),\n",
        "    ctypes.c_int, ctypes.c_int\n",
        "]\n",
        "cuda_lib.softmax_cuda.restype = ctypes.c_int\n",
        "\n",
        "# Initialize CUDA\n",
        "print(f'CUDA Device: {cuda_lib.cuda_device_name().decode()}')\n",
        "cuda_lib.cuda_init(100_000_000, 10_000_000, 10_000_000)\n",
        "print('CUDA initialized')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Benchmark: EdgeLLM CUDA Kernels\n",
        "\n",
        "Simulate inference workload with realistic dimensions:\n",
        "- SmolLM-135M: hidden_size=576, intermediate=1536, vocab=49152"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark_cuda_inference(batch_size=1, seq_len=1, hidden_size=576, \n",
        "                              intermediate_size=1536, vocab_size=49152,\n",
        "                              num_layers=9, num_tokens=100):\n",
        "    \"\"\"\n",
        "    Simulate transformer inference with CUDA kernels.\n",
        "    \n",
        "    Per token operations:\n",
        "    - RMSNorm x 2 per layer\n",
        "    - QKV projection (matmul)\n",
        "    - Attention output projection\n",
        "    - MLP up projection\n",
        "    - MLP down projection\n",
        "    - Final LM head\n",
        "    \"\"\"\n",
        "    # Allocate buffers\n",
        "    hidden = np.random.randn(batch_size * seq_len, hidden_size).astype(np.float32)\n",
        "    weights_qkv = np.random.randint(-1, 2, (hidden_size * 3, hidden_size // 4), dtype=np.int8)\n",
        "    weights_out = np.random.randint(-1, 2, (hidden_size, hidden_size // 4), dtype=np.int8)\n",
        "    weights_up = np.random.randint(-1, 2, (intermediate_size, hidden_size // 4), dtype=np.int8)\n",
        "    weights_down = np.random.randint(-1, 2, (hidden_size, intermediate_size // 4), dtype=np.int8)\n",
        "    weights_lm = np.random.randint(-1, 2, (vocab_size, hidden_size // 4), dtype=np.int8)\n",
        "    \n",
        "    norm_weight = np.ones(hidden_size, dtype=np.float32)\n",
        "    scales = np.ones(max(hidden_size * 3, intermediate_size, vocab_size), dtype=np.float32)\n",
        "    \n",
        "    # Output buffers\n",
        "    output_qkv = np.zeros((batch_size * seq_len, hidden_size * 3), dtype=np.float32)\n",
        "    output_out = np.zeros((batch_size * seq_len, hidden_size), dtype=np.float32)\n",
        "    output_up = np.zeros((batch_size * seq_len, intermediate_size), dtype=np.float32)\n",
        "    output_down = np.zeros((batch_size * seq_len, hidden_size), dtype=np.float32)\n",
        "    output_lm = np.zeros((batch_size * seq_len, vocab_size), dtype=np.float32)\n",
        "    norm_out = np.zeros_like(hidden)\n",
        "    \n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        cuda_lib.rmsnorm_cuda(\n",
        "            norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            hidden.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            norm_weight.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            batch_size * seq_len, hidden_size, ctypes.c_float(1e-6)\n",
        "        )\n",
        "    cuda_lib.cuda_sync()\n",
        "    \n",
        "    # Benchmark\n",
        "    start = time.perf_counter()\n",
        "    \n",
        "    for token in range(num_tokens):\n",
        "        for layer in range(num_layers):\n",
        "            # Pre-attention RMSNorm\n",
        "            cuda_lib.rmsnorm_cuda(\n",
        "                norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                hidden.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                norm_weight.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                batch_size * seq_len, hidden_size, ctypes.c_float(1e-6)\n",
        "            )\n",
        "            \n",
        "            # QKV projection (simulated with matmul)\n",
        "            cuda_lib.tmac_matmul_cuda(\n",
        "                weights_qkv.ctypes.data_as(ctypes.POINTER(ctypes.c_int8)),\n",
        "                norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                output_qkv.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                hidden_size * 3, batch_size * seq_len, hidden_size\n",
        "            )\n",
        "            \n",
        "            # Attention output\n",
        "            cuda_lib.tmac_matmul_cuda(\n",
        "                weights_out.ctypes.data_as(ctypes.POINTER(ctypes.c_int8)),\n",
        "                output_qkv.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                output_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                hidden_size, batch_size * seq_len, hidden_size * 3\n",
        "            )\n",
        "            \n",
        "            # Post-attention RMSNorm\n",
        "            cuda_lib.rmsnorm_cuda(\n",
        "                norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                output_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                norm_weight.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                batch_size * seq_len, hidden_size, ctypes.c_float(1e-6)\n",
        "            )\n",
        "            \n",
        "            # MLP up\n",
        "            cuda_lib.tmac_matmul_cuda(\n",
        "                weights_up.ctypes.data_as(ctypes.POINTER(ctypes.c_int8)),\n",
        "                norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                output_up.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                intermediate_size, batch_size * seq_len, hidden_size\n",
        "            )\n",
        "            \n",
        "            # MLP down\n",
        "            cuda_lib.tmac_matmul_cuda(\n",
        "                weights_down.ctypes.data_as(ctypes.POINTER(ctypes.c_int8)),\n",
        "                output_up.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                output_down.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "                hidden_size, batch_size * seq_len, intermediate_size\n",
        "            )\n",
        "        \n",
        "        # Final LM head\n",
        "        cuda_lib.rmsnorm_cuda(\n",
        "            norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            output_down.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            norm_weight.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            batch_size * seq_len, hidden_size, ctypes.c_float(1e-6)\n",
        "        )\n",
        "        \n",
        "        cuda_lib.tmac_matmul_cuda(\n",
        "            weights_lm.ctypes.data_as(ctypes.POINTER(ctypes.c_int8)),\n",
        "            norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            output_lm.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            vocab_size, batch_size * seq_len, hidden_size\n",
        "        )\n",
        "        \n",
        "        # Softmax for sampling\n",
        "        cuda_lib.softmax_cuda(\n",
        "            output_lm.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            output_lm.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "            batch_size * seq_len, vocab_size\n",
        "        )\n",
        "    \n",
        "    cuda_lib.cuda_sync()\n",
        "    end = time.perf_counter()\n",
        "    \n",
        "    elapsed = end - start\n",
        "    tok_per_sec = num_tokens / elapsed\n",
        "    \n",
        "    return {\n",
        "        'tokens': num_tokens,\n",
        "        'elapsed_sec': elapsed,\n",
        "        'tok_per_sec': tok_per_sec,\n",
        "        'ms_per_token': (elapsed / num_tokens) * 1000\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run EdgeLLM CUDA benchmark\n",
        "print('EdgeLLM CUDA Benchmark (SmolLM-135M simulation)')\n",
        "print('=' * 50)\n",
        "\n",
        "cuda_results = benchmark_cuda_inference(num_tokens=100)\n",
        "\n",
        "print(f\"Tokens generated: {cuda_results['tokens']}\")\n",
        "print(f\"Total time: {cuda_results['elapsed_sec']:.3f}s\")\n",
        "print(f\"Throughput: {cuda_results['tok_per_sec']:.1f} tok/s\")\n",
        "print(f\"Latency: {cuda_results['ms_per_token']:.2f} ms/token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Benchmark: Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def benchmark_ollama(model='smollm:135m', prompt='Write a story about', num_tokens=100):\n",
        "    \"\"\"\n",
        "    Benchmark Ollama inference.\n",
        "    \"\"\"\n",
        "    url = 'http://localhost:11434/api/generate'\n",
        "    \n",
        "    payload = {\n",
        "        'model': model,\n",
        "        'prompt': prompt,\n",
        "        'stream': False,\n",
        "        'options': {\n",
        "            'num_predict': num_tokens,\n",
        "            'temperature': 0.7\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    start = time.perf_counter()\n",
        "    response = requests.post(url, json=payload)\n",
        "    end = time.perf_counter()\n",
        "    \n",
        "    if response.status_code != 200:\n",
        "        print(f'Error: {response.status_code}')\n",
        "        print(response.text)\n",
        "        return None\n",
        "    \n",
        "    data = response.json()\n",
        "    \n",
        "    # Extract metrics from Ollama response\n",
        "    eval_count = data.get('eval_count', num_tokens)\n",
        "    eval_duration = data.get('eval_duration', 0) / 1e9  # nanoseconds to seconds\n",
        "    \n",
        "    if eval_duration > 0:\n",
        "        tok_per_sec = eval_count / eval_duration\n",
        "    else:\n",
        "        elapsed = end - start\n",
        "        tok_per_sec = eval_count / elapsed\n",
        "        eval_duration = elapsed\n",
        "    \n",
        "    return {\n",
        "        'tokens': eval_count,\n",
        "        'elapsed_sec': eval_duration,\n",
        "        'tok_per_sec': tok_per_sec,\n",
        "        'ms_per_token': (eval_duration / eval_count) * 1000 if eval_count > 0 else 0,\n",
        "        'response': data.get('response', '')[:100]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Ollama benchmark\n",
        "print('Ollama Benchmark (SmolLM-135M)')\n",
        "print('=' * 50)\n",
        "\n",
        "ollama_results = benchmark_ollama(num_tokens=100)\n",
        "\n",
        "if ollama_results:\n",
        "    print(f\"Tokens generated: {ollama_results['tokens']}\")\n",
        "    print(f\"Total time: {ollama_results['elapsed_sec']:.3f}s\")\n",
        "    print(f\"Throughput: {ollama_results['tok_per_sec']:.1f} tok/s\")\n",
        "    print(f\"Latency: {ollama_results['ms_per_token']:.2f} ms/token\")\n",
        "    print(f\"\\nSample output: {ollama_results['response']}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Head-to-Head Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '=' * 60)\n",
        "print('BENCHMARK COMPARISON: EdgeLLM CUDA vs Ollama')\n",
        "print('=' * 60)\n",
        "print(f\"{'Metric':<25} {'EdgeLLM CUDA':>15} {'Ollama':>15} {'Winner':>10}\")\n",
        "print('-' * 60)\n",
        "\n",
        "if ollama_results:\n",
        "    # Throughput\n",
        "    cuda_tps = cuda_results['tok_per_sec']\n",
        "    ollama_tps = ollama_results['tok_per_sec']\n",
        "    winner_tps = 'EdgeLLM' if cuda_tps > ollama_tps else 'Ollama'\n",
        "    print(f\"{'Throughput (tok/s)':<25} {cuda_tps:>15.1f} {ollama_tps:>15.1f} {winner_tps:>10}\")\n",
        "    \n",
        "    # Latency\n",
        "    cuda_lat = cuda_results['ms_per_token']\n",
        "    ollama_lat = ollama_results['ms_per_token']\n",
        "    winner_lat = 'EdgeLLM' if cuda_lat < ollama_lat else 'Ollama'\n",
        "    print(f\"{'Latency (ms/token)':<25} {cuda_lat:>15.2f} {ollama_lat:>15.2f} {winner_lat:>10}\")\n",
        "    \n",
        "    # Speedup\n",
        "    speedup = cuda_tps / ollama_tps if ollama_tps > 0 else 0\n",
        "    print('-' * 60)\n",
        "    print(f\"{'Speedup':<25} {speedup:>15.2f}x\")\n",
        "else:\n",
        "    print('Ollama benchmark failed - showing EdgeLLM results only')\n",
        "    print(f\"{'Throughput (tok/s)':<25} {cuda_results['tok_per_sec']:>15.1f}\")\n",
        "    print(f\"{'Latency (ms/token)':<25} {cuda_results['ms_per_token']:>15.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Extended Benchmark (Multiple Runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run multiple iterations for statistical significance\n",
        "print('Running extended benchmark (5 iterations)...')\n",
        "print()\n",
        "\n",
        "cuda_runs = []\n",
        "ollama_runs = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'Run {i+1}/5...')\n",
        "    \n",
        "    # EdgeLLM CUDA\n",
        "    r = benchmark_cuda_inference(num_tokens=50)\n",
        "    cuda_runs.append(r['tok_per_sec'])\n",
        "    \n",
        "    # Ollama\n",
        "    r = benchmark_ollama(num_tokens=50)\n",
        "    if r:\n",
        "        ollama_runs.append(r['tok_per_sec'])\n",
        "\n",
        "print()\n",
        "print('Extended Results:')\n",
        "print(f\"EdgeLLM CUDA: {np.mean(cuda_runs):.1f} ± {np.std(cuda_runs):.1f} tok/s\")\n",
        "if ollama_runs:\n",
        "    print(f\"Ollama:       {np.mean(ollama_runs):.1f} ± {np.std(ollama_runs):.1f} tok/s\")\n",
        "    print(f\"\\nSpeedup: {np.mean(cuda_runs) / np.mean(ollama_runs):.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "cuda_lib.cuda_cleanup()\n",
        "ollama_process.terminate()\n",
        "print('Cleanup complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **EdgeLLM CUDA** uses T-MAC (table lookup) instead of multiply-accumulate\n",
        "2. **BitNet 1.58-bit** quantization provides 6.5x compression\n",
        "3. **CUDA kernels** leverage shared memory for fast LUT access\n",
        "\n",
        "### Why EdgeLLM Can Be Faster:\n",
        "\n",
        "- **No FP16/FP32 multiplications** - just table lookups\n",
        "- **Smaller model size** - fits in L2 cache\n",
        "- **Lower memory bandwidth** - main bottleneck for LLMs\n",
        "- **Deterministic latency** - no GC pauses (Mojo)\n",
        "\n",
        "### Target Performance:\n",
        "\n",
        "| Hardware | EdgeLLM Target | Ollama Typical |\n",
        "|----------|----------------|----------------|\n",
        "| Tesla T4 | 200-400 tok/s | 100-150 tok/s |\n",
        "| RTX 3090 | 400-600 tok/s | 150-200 tok/s |\n",
        "| Jetson Nano | 80-120 tok/s | 20-40 tok/s |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
